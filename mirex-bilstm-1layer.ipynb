{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7070943,"sourceType":"datasetVersion","datasetId":4071995}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"id":"24AHW8N34NwS","execution":{"iopub.status.busy":"2023-11-29T06:02:41.474511Z","iopub.execute_input":"2023-11-29T06:02:41.474992Z","iopub.status.idle":"2023-11-29T06:02:41.479850Z","shell.execute_reply.started":"2023-11-29T06:02:41.474953Z","shell.execute_reply":"2023-11-29T06:02:41.478835Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mirex-dataset/dataset_info.csv')","metadata":{"id":"gZtQmQ2VAN2p","execution":{"iopub.status.busy":"2023-11-29T06:02:41.482640Z","iopub.execute_input":"2023-11-29T06:02:41.483324Z","iopub.status.idle":"2023-11-29T06:02:41.510087Z","shell.execute_reply.started":"2023-11-29T06:02:41.483270Z","shell.execute_reply":"2023-11-29T06:02:41.509215Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"SwbQqkQYsq2x","outputId":"5016499f-a970-4c9e-cbe1-419c95b286a7","execution":{"iopub.status.busy":"2023-11-29T06:02:41.511268Z","iopub.execute_input":"2023-11-29T06:02:41.511584Z","iopub.status.idle":"2023-11-29T06:02:41.527460Z","shell.execute_reply.started":"2023-11-29T06:02:41.511561Z","shell.execute_reply":"2023-11-29T06:02:41.526470Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0                                Title              Artist  \\\n0             0  (Mama) He Treats Your Daughter Mean         Brown, Ruth   \n1             1                     Along Came Jones            Coasters   \n2             2                        Back in Black               AC/DC   \n3             3  Burn Rubber (Why You Wanna Hurt Me)        Gap Band [1]   \n4             4               Cigarettes and Alcohol               Oasis   \n..          ...                                  ...                 ...   \n759         759                 Smokestack Lightning        Howlin' Wolf   \n760         760                            I'm a Man           Yardbirds   \n761         761                       Blow Your Mind              Redman   \n762         762                        Paradise City        Guns N Roses   \n763         763         Deutschland (Has Gotta Die!)  Atari Teenage Riot   \n\n                                     Album Audio_Filename  \\\n0               Best of Ruth Brown [Rhino]        001.mp3   \n1          50 Coastin' Classics: Anthology        003.mp3   \n2                            Back in Black        004.mp3   \n3              Billboard Hot R&B Hits 1981        007.mp3   \n4                         Definitely Maybe        008.mp3   \n..                                     ...            ...   \n759  Howlin' Wolf/Moanin' in the Moonlight        899.mp3   \n760       Greatest Hits, Vol. 1: 1964-1966        900.mp3   \n761                       Whut? Thee Album        901.mp3   \n762               Appetite for Destruction        902.mp3   \n763                     Burn, Berlin, Burn        903.mp3   \n\n                                                Lyrics    Category    Cluster  \n0    Mama he treats your daughter mean \\nMama he tr...  Boisterous  Cluster 1  \n1    I plopped down in my easy chair and turned on ...  Boisterous  Cluster 1  \n2    Back in black \\nI hit the sack \\nIt's been too...  Boisterous  Cluster 1  \n3    Woo, I gave you my money, I gave you my time.\\...  Boisterous  Cluster 1  \n4    Is it my imagination \\nOr have I finally found...  Boisterous  Cluster 1  \n..                                                 ...         ...        ...  \n759  (Chester Burnett a.k.a. Howlin' Wolf)\\nAh-oh, ...   Agressive  Cluster 5  \n760  You pretty women,\\nStanding in line,\\nMake lov...   Agressive  Cluster 5  \n761  AHHHH!! Look out  it's the Funkadelic Funk for...   Agressive  Cluster 5  \n762  [Chorus: x2]\\n\\nTake me down\\nTo the paradise ...       Fiery  Cluster 5  \n763  Deutschland has gotta die! (eins, zwei, drei, ...       Fiery  Cluster 5  \n\n[764 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Title</th>\n      <th>Artist</th>\n      <th>Album</th>\n      <th>Audio_Filename</th>\n      <th>Lyrics</th>\n      <th>Category</th>\n      <th>Cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>(Mama) He Treats Your Daughter Mean</td>\n      <td>Brown, Ruth</td>\n      <td>Best of Ruth Brown [Rhino]</td>\n      <td>001.mp3</td>\n      <td>Mama he treats your daughter mean \\nMama he tr...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Along Came Jones</td>\n      <td>Coasters</td>\n      <td>50 Coastin' Classics: Anthology</td>\n      <td>003.mp3</td>\n      <td>I plopped down in my easy chair and turned on ...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Back in Black</td>\n      <td>AC/DC</td>\n      <td>Back in Black</td>\n      <td>004.mp3</td>\n      <td>Back in black \\nI hit the sack \\nIt's been too...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Burn Rubber (Why You Wanna Hurt Me)</td>\n      <td>Gap Band [1]</td>\n      <td>Billboard Hot R&amp;B Hits 1981</td>\n      <td>007.mp3</td>\n      <td>Woo, I gave you my money, I gave you my time.\\...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Cigarettes and Alcohol</td>\n      <td>Oasis</td>\n      <td>Definitely Maybe</td>\n      <td>008.mp3</td>\n      <td>Is it my imagination \\nOr have I finally found...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>759</th>\n      <td>759</td>\n      <td>Smokestack Lightning</td>\n      <td>Howlin' Wolf</td>\n      <td>Howlin' Wolf/Moanin' in the Moonlight</td>\n      <td>899.mp3</td>\n      <td>(Chester Burnett a.k.a. Howlin' Wolf)\\nAh-oh, ...</td>\n      <td>Agressive</td>\n      <td>Cluster 5</td>\n    </tr>\n    <tr>\n      <th>760</th>\n      <td>760</td>\n      <td>I'm a Man</td>\n      <td>Yardbirds</td>\n      <td>Greatest Hits, Vol. 1: 1964-1966</td>\n      <td>900.mp3</td>\n      <td>You pretty women,\\nStanding in line,\\nMake lov...</td>\n      <td>Agressive</td>\n      <td>Cluster 5</td>\n    </tr>\n    <tr>\n      <th>761</th>\n      <td>761</td>\n      <td>Blow Your Mind</td>\n      <td>Redman</td>\n      <td>Whut? Thee Album</td>\n      <td>901.mp3</td>\n      <td>AHHHH!! Look out  it's the Funkadelic Funk for...</td>\n      <td>Agressive</td>\n      <td>Cluster 5</td>\n    </tr>\n    <tr>\n      <th>762</th>\n      <td>762</td>\n      <td>Paradise City</td>\n      <td>Guns N Roses</td>\n      <td>Appetite for Destruction</td>\n      <td>902.mp3</td>\n      <td>[Chorus: x2]\\n\\nTake me down\\nTo the paradise ...</td>\n      <td>Fiery</td>\n      <td>Cluster 5</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>763</td>\n      <td>Deutschland (Has Gotta Die!)</td>\n      <td>Atari Teenage Riot</td>\n      <td>Burn, Berlin, Burn</td>\n      <td>903.mp3</td>\n      <td>Deutschland has gotta die! (eins, zwei, drei, ...</td>\n      <td>Fiery</td>\n      <td>Cluster 5</td>\n    </tr>\n  </tbody>\n</table>\n<p>764 rows Ã— 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['Lyrics'] = df['Lyrics'].map(lambda x: x.replace('\\n',' '))\ndf","metadata":{"id":"IeJZemiEATOY","outputId":"1af9fd2b-7e9c-4df6-c5f5-9a9953e6051c","execution":{"iopub.status.busy":"2023-11-29T06:02:41.528656Z","iopub.execute_input":"2023-11-29T06:02:41.529011Z","iopub.status.idle":"2023-11-29T06:02:41.552411Z","shell.execute_reply.started":"2023-11-29T06:02:41.528979Z","shell.execute_reply":"2023-11-29T06:02:41.551574Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0                                Title              Artist  \\\n0             0  (Mama) He Treats Your Daughter Mean         Brown, Ruth   \n1             1                     Along Came Jones            Coasters   \n2             2                        Back in Black               AC/DC   \n3             3  Burn Rubber (Why You Wanna Hurt Me)        Gap Band [1]   \n4             4               Cigarettes and Alcohol               Oasis   \n..          ...                                  ...                 ...   \n759         759                 Smokestack Lightning        Howlin' Wolf   \n760         760                            I'm a Man           Yardbirds   \n761         761                       Blow Your Mind              Redman   \n762         762                        Paradise City        Guns N Roses   \n763         763         Deutschland (Has Gotta Die!)  Atari Teenage Riot   \n\n                                     Album Audio_Filename  \\\n0               Best of Ruth Brown [Rhino]        001.mp3   \n1          50 Coastin' Classics: Anthology        003.mp3   \n2                            Back in Black        004.mp3   \n3              Billboard Hot R&B Hits 1981        007.mp3   \n4                         Definitely Maybe        008.mp3   \n..                                     ...            ...   \n759  Howlin' Wolf/Moanin' in the Moonlight        899.mp3   \n760       Greatest Hits, Vol. 1: 1964-1966        900.mp3   \n761                       Whut? Thee Album        901.mp3   \n762               Appetite for Destruction        902.mp3   \n763                     Burn, Berlin, Burn        903.mp3   \n\n                                                Lyrics    Category    Cluster  \n0    Mama he treats your daughter mean  Mama he tre...  Boisterous  Cluster 1  \n1    I plopped down in my easy chair and turned on ...  Boisterous  Cluster 1  \n2    Back in black  I hit the sack  It's been too l...  Boisterous  Cluster 1  \n3    Woo, I gave you my money, I gave you my time. ...  Boisterous  Cluster 1  \n4    Is it my imagination  Or have I finally found ...  Boisterous  Cluster 1  \n..                                                 ...         ...        ...  \n759  (Chester Burnett a.k.a. Howlin' Wolf) Ah-oh, s...   Agressive  Cluster 5  \n760  You pretty women, Standing in line, Make love ...   Agressive  Cluster 5  \n761  AHHHH!! Look out  it's the Funkadelic Funk for...   Agressive  Cluster 5  \n762  [Chorus: x2]  Take me down To the paradise cit...       Fiery  Cluster 5  \n763  Deutschland has gotta die! (eins, zwei, drei, ...       Fiery  Cluster 5  \n\n[764 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Title</th>\n      <th>Artist</th>\n      <th>Album</th>\n      <th>Audio_Filename</th>\n      <th>Lyrics</th>\n      <th>Category</th>\n      <th>Cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>(Mama) He Treats Your Daughter Mean</td>\n      <td>Brown, Ruth</td>\n      <td>Best of Ruth Brown [Rhino]</td>\n      <td>001.mp3</td>\n      <td>Mama he treats your daughter mean  Mama he tre...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Along Came Jones</td>\n      <td>Coasters</td>\n      <td>50 Coastin' Classics: Anthology</td>\n      <td>003.mp3</td>\n      <td>I plopped down in my easy chair and turned on ...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Back in Black</td>\n      <td>AC/DC</td>\n      <td>Back in Black</td>\n      <td>004.mp3</td>\n      <td>Back in black  I hit the sack  It's been too l...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Burn Rubber (Why You Wanna Hurt Me)</td>\n      <td>Gap Band [1]</td>\n      <td>Billboard Hot R&amp;B Hits 1981</td>\n      <td>007.mp3</td>\n      <td>Woo, I gave you my money, I gave you my time. ...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Cigarettes and Alcohol</td>\n      <td>Oasis</td>\n      <td>Definitely Maybe</td>\n      <td>008.mp3</td>\n      <td>Is it my imagination  Or have I finally found ...</td>\n      <td>Boisterous</td>\n      <td>Cluster 1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>759</th>\n      <td>759</td>\n      <td>Smokestack Lightning</td>\n      <td>Howlin' Wolf</td>\n      <td>Howlin' Wolf/Moanin' in the Moonlight</td>\n      <td>899.mp3</td>\n      <td>(Chester Burnett a.k.a. Howlin' Wolf) Ah-oh, s...</td>\n      <td>Agressive</td>\n      <td>Cluster 5</td>\n    </tr>\n    <tr>\n      <th>760</th>\n      <td>760</td>\n      <td>I'm a Man</td>\n      <td>Yardbirds</td>\n      <td>Greatest Hits, Vol. 1: 1964-1966</td>\n      <td>900.mp3</td>\n      <td>You pretty women, Standing in line, Make love ...</td>\n      <td>Agressive</td>\n      <td>Cluster 5</td>\n    </tr>\n    <tr>\n      <th>761</th>\n      <td>761</td>\n      <td>Blow Your Mind</td>\n      <td>Redman</td>\n      <td>Whut? Thee Album</td>\n      <td>901.mp3</td>\n      <td>AHHHH!! Look out  it's the Funkadelic Funk for...</td>\n      <td>Agressive</td>\n      <td>Cluster 5</td>\n    </tr>\n    <tr>\n      <th>762</th>\n      <td>762</td>\n      <td>Paradise City</td>\n      <td>Guns N Roses</td>\n      <td>Appetite for Destruction</td>\n      <td>902.mp3</td>\n      <td>[Chorus: x2]  Take me down To the paradise cit...</td>\n      <td>Fiery</td>\n      <td>Cluster 5</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>763</td>\n      <td>Deutschland (Has Gotta Die!)</td>\n      <td>Atari Teenage Riot</td>\n      <td>Burn, Berlin, Burn</td>\n      <td>903.mp3</td>\n      <td>Deutschland has gotta die! (eins, zwei, drei, ...</td>\n      <td>Fiery</td>\n      <td>Cluster 5</td>\n    </tr>\n  </tbody>\n</table>\n<p>764 rows Ã— 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"all_emotions = df['Category'].unique()\nprint(len(all_emotions))","metadata":{"id":"7MPxxCAns3Gp","outputId":"5d6da544-c43f-44ef-dbf6-fdcc2c85bf1f","execution":{"iopub.status.busy":"2023-11-29T06:02:41.556467Z","iopub.execute_input":"2023-11-29T06:02:41.556781Z","iopub.status.idle":"2023-11-29T06:02:41.561663Z","shell.execute_reply.started":"2023-11-29T06:02:41.556758Z","shell.execute_reply":"2023-11-29T06:02:41.560687Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"24\n","output_type":"stream"}]},{"cell_type":"code","source":"df1 = pd.get_dummies(df['Category'],dtype='int')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:02:41.562662Z","iopub.execute_input":"2023-11-29T06:02:41.562922Z","iopub.status.idle":"2023-11-29T06:02:41.573314Z","shell.execute_reply.started":"2023-11-29T06:02:41.562900Z","shell.execute_reply":"2023-11-29T06:02:41.572168Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"idx_to_emotions = dict()\nemotions_to_idx = dict()\nfor i, j in enumerate(all_emotions):\n    idx_to_emotions[i] = j\n    emotions_to_idx[j] = i","metadata":{"id":"EV6Z5-k_tJYV","execution":{"iopub.status.busy":"2023-11-29T06:02:41.574541Z","iopub.execute_input":"2023-11-29T06:02:41.574821Z","iopub.status.idle":"2023-11-29T06:02:41.582678Z","shell.execute_reply.started":"2023-11-29T06:02:41.574796Z","shell.execute_reply":"2023-11-29T06:02:41.581695Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"df['Category'] = df['Category'].map(lambda x: emotions_to_idx[x])","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:02:41.583934Z","iopub.execute_input":"2023-11-29T06:02:41.584853Z","iopub.status.idle":"2023-11-29T06:02:41.594948Z","shell.execute_reply.started":"2023-11-29T06:02:41.584824Z","shell.execute_reply":"2023-11-29T06:02:41.593949Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df['Category']","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:02:41.595897Z","iopub.execute_input":"2023-11-29T06:02:41.596160Z","iopub.status.idle":"2023-11-29T06:02:41.607574Z","shell.execute_reply.started":"2023-11-29T06:02:41.596133Z","shell.execute_reply":"2023-11-29T06:02:41.606752Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"0       0\n1       0\n2       0\n3       0\n4       0\n       ..\n759    22\n760    22\n761    22\n762    23\n763    23\nName: Category, Length: 764, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"X = df['Lyrics']\ny = df['Category']","metadata":{"id":"liTqjmpCEUm9","execution":{"iopub.status.busy":"2023-11-29T06:02:41.720440Z","iopub.execute_input":"2023-11-29T06:02:41.720685Z","iopub.status.idle":"2023-11-29T06:02:41.724683Z","shell.execute_reply.started":"2023-11-29T06:02:41.720664Z","shell.execute_reply":"2023-11-29T06:02:41.723767Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"clusters = df['Cluster'].unique()\nclusters","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:43:22.574645Z","iopub.execute_input":"2023-11-29T06:43:22.575030Z","iopub.status.idle":"2023-11-29T06:43:22.582966Z","shell.execute_reply.started":"2023-11-29T06:43:22.574999Z","shell.execute_reply":"2023-11-29T06:43:22.581714Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"array(['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"cluster_to_idx = dict()\nidx_to_cluster = dict()\nfor i in range(0, len(clusters)):\n    cluster_to_idx[clusters[i]] = i\n    idx_to_cluster[i] = clusters[i]\ncluster_to_idx","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:45:13.724976Z","iopub.execute_input":"2023-11-29T06:45:13.725399Z","iopub.status.idle":"2023-11-29T06:45:13.733085Z","shell.execute_reply.started":"2023-11-29T06:45:13.725368Z","shell.execute_reply":"2023-11-29T06:45:13.732094Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"{'Cluster 1': 0,\n 'Cluster 2': 1,\n 'Cluster 3': 2,\n 'Cluster 4': 3,\n 'Cluster 5': 4}"},"metadata":{}}]},{"cell_type":"code","source":"df['Cluster'] = df['Cluster'].map(lambda x: cluster_to_idx[x])","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:45:48.354912Z","iopub.execute_input":"2023-11-29T06:45:48.355352Z","iopub.status.idle":"2023-11-29T06:45:48.361599Z","shell.execute_reply.started":"2023-11-29T06:45:48.355314Z","shell.execute_reply":"2023-11-29T06:45:48.360547Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:45:52.741744Z","iopub.execute_input":"2023-11-29T06:45:52.742105Z","iopub.status.idle":"2023-11-29T06:45:52.762486Z","shell.execute_reply.started":"2023-11-29T06:45:52.742075Z","shell.execute_reply":"2023-11-29T06:45:52.761428Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0                                Title              Artist  \\\n0             0  (Mama) He Treats Your Daughter Mean         Brown, Ruth   \n1             1                     Along Came Jones            Coasters   \n2             2                        Back in Black               AC/DC   \n3             3  Burn Rubber (Why You Wanna Hurt Me)        Gap Band [1]   \n4             4               Cigarettes and Alcohol               Oasis   \n..          ...                                  ...                 ...   \n759         759                 Smokestack Lightning        Howlin' Wolf   \n760         760                            I'm a Man           Yardbirds   \n761         761                       Blow Your Mind              Redman   \n762         762                        Paradise City        Guns N Roses   \n763         763         Deutschland (Has Gotta Die!)  Atari Teenage Riot   \n\n                                     Album Audio_Filename  \\\n0               Best of Ruth Brown [Rhino]        001.mp3   \n1          50 Coastin' Classics: Anthology        003.mp3   \n2                            Back in Black        004.mp3   \n3              Billboard Hot R&B Hits 1981        007.mp3   \n4                         Definitely Maybe        008.mp3   \n..                                     ...            ...   \n759  Howlin' Wolf/Moanin' in the Moonlight        899.mp3   \n760       Greatest Hits, Vol. 1: 1964-1966        900.mp3   \n761                       Whut? Thee Album        901.mp3   \n762               Appetite for Destruction        902.mp3   \n763                     Burn, Berlin, Burn        903.mp3   \n\n                                                Lyrics  Category  Cluster  \n0    Mama he treats your daughter mean  Mama he tre...         0        0  \n1    I plopped down in my easy chair and turned on ...         0        0  \n2    Back in black  I hit the sack  It's been too l...         0        0  \n3    Woo, I gave you my money, I gave you my time. ...         0        0  \n4    Is it my imagination  Or have I finally found ...         0        0  \n..                                                 ...       ...      ...  \n759  (Chester Burnett a.k.a. Howlin' Wolf) Ah-oh, s...        22        4  \n760  You pretty women, Standing in line, Make love ...        22        4  \n761  AHHHH!! Look out  it's the Funkadelic Funk for...        22        4  \n762  [Chorus: x2]  Take me down To the paradise cit...        23        4  \n763  Deutschland has gotta die! (eins, zwei, drei, ...        23        4  \n\n[764 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Title</th>\n      <th>Artist</th>\n      <th>Album</th>\n      <th>Audio_Filename</th>\n      <th>Lyrics</th>\n      <th>Category</th>\n      <th>Cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>(Mama) He Treats Your Daughter Mean</td>\n      <td>Brown, Ruth</td>\n      <td>Best of Ruth Brown [Rhino]</td>\n      <td>001.mp3</td>\n      <td>Mama he treats your daughter mean  Mama he tre...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Along Came Jones</td>\n      <td>Coasters</td>\n      <td>50 Coastin' Classics: Anthology</td>\n      <td>003.mp3</td>\n      <td>I plopped down in my easy chair and turned on ...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Back in Black</td>\n      <td>AC/DC</td>\n      <td>Back in Black</td>\n      <td>004.mp3</td>\n      <td>Back in black  I hit the sack  It's been too l...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Burn Rubber (Why You Wanna Hurt Me)</td>\n      <td>Gap Band [1]</td>\n      <td>Billboard Hot R&amp;B Hits 1981</td>\n      <td>007.mp3</td>\n      <td>Woo, I gave you my money, I gave you my time. ...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Cigarettes and Alcohol</td>\n      <td>Oasis</td>\n      <td>Definitely Maybe</td>\n      <td>008.mp3</td>\n      <td>Is it my imagination  Or have I finally found ...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>759</th>\n      <td>759</td>\n      <td>Smokestack Lightning</td>\n      <td>Howlin' Wolf</td>\n      <td>Howlin' Wolf/Moanin' in the Moonlight</td>\n      <td>899.mp3</td>\n      <td>(Chester Burnett a.k.a. Howlin' Wolf) Ah-oh, s...</td>\n      <td>22</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>760</th>\n      <td>760</td>\n      <td>I'm a Man</td>\n      <td>Yardbirds</td>\n      <td>Greatest Hits, Vol. 1: 1964-1966</td>\n      <td>900.mp3</td>\n      <td>You pretty women, Standing in line, Make love ...</td>\n      <td>22</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>761</th>\n      <td>761</td>\n      <td>Blow Your Mind</td>\n      <td>Redman</td>\n      <td>Whut? Thee Album</td>\n      <td>901.mp3</td>\n      <td>AHHHH!! Look out  it's the Funkadelic Funk for...</td>\n      <td>22</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>762</th>\n      <td>762</td>\n      <td>Paradise City</td>\n      <td>Guns N Roses</td>\n      <td>Appetite for Destruction</td>\n      <td>902.mp3</td>\n      <td>[Chorus: x2]  Take me down To the paradise cit...</td>\n      <td>23</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>763</td>\n      <td>Deutschland (Has Gotta Die!)</td>\n      <td>Atari Teenage Riot</td>\n      <td>Burn, Berlin, Burn</td>\n      <td>903.mp3</td>\n      <td>Deutschland has gotta die! (eins, zwei, drei, ...</td>\n      <td>23</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>764 rows Ã— 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=56)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=56)","metadata":{"id":"sZcP6_SCEjNJ","execution":{"iopub.status.busy":"2023-11-29T06:02:41.731096Z","iopub.execute_input":"2023-11-29T06:02:41.731936Z","iopub.status.idle":"2023-11-29T06:02:41.740435Z","shell.execute_reply.started":"2023-11-29T06:02:41.731904Z","shell.execute_reply":"2023-11-29T06:02:41.739700Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"X_train1, X_test1, y_train1, y_test1 = train_test_split(X, df['Cluster'], test_size=0.3, random_state=56)\n\nX_train1, X_val11, y_train, y_val1 = train_test_split(X_train1, y_train1, test_size=0.25, random_state=56)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:46:46.250607Z","iopub.execute_input":"2023-11-29T06:46:46.251471Z","iopub.status.idle":"2023-11-29T06:46:46.260601Z","shell.execute_reply.started":"2023-11-29T06:46:46.251438Z","shell.execute_reply":"2023-11-29T06:46:46.259692Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence\nfrom torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau\nimport numpy as np","metadata":{"id":"ufwuiNRqElPr","execution":{"iopub.status.busy":"2023-11-29T06:02:41.742102Z","iopub.execute_input":"2023-11-29T06:02:41.742390Z","iopub.status.idle":"2023-11-29T06:02:41.759047Z","shell.execute_reply.started":"2023-11-29T06:02:41.742368Z","shell.execute_reply":"2023-11-29T06:02:41.757983Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"_5yac1g9Epzr","execution":{"iopub.status.busy":"2023-11-29T06:02:41.759961Z","iopub.execute_input":"2023-11-29T06:02:41.760240Z","iopub.status.idle":"2023-11-29T06:02:41.770478Z","shell.execute_reply.started":"2023-11-29T06:02:41.760206Z","shell.execute_reply":"2023-11-29T06:02:41.769706Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"id":"IS9ystxvEpxy","outputId":"0acb6bbc-83d2-43e5-a98e-106b71925221","execution":{"iopub.status.busy":"2023-11-29T06:02:41.771602Z","iopub.execute_input":"2023-11-29T06:02:41.771946Z","iopub.status.idle":"2023-11-29T06:02:41.788111Z","shell.execute_reply.started":"2023-11-29T06:02:41.771888Z","shell.execute_reply":"2023-11-29T06:02:41.787269Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"all_words_vocab = dict()\nall_words_vocab['pad'] = 0\nall_words_vocab['unk'] = 1\ncount = 2\n\nfor i in X_train:\n    for j in i.split(' '):\n        if j not in all_words_vocab.keys():\n            all_words_vocab[j] = count\n            count += 1\nprint(len(all_words_vocab))","metadata":{"id":"lkDcoOg5Epw4","outputId":"4cafe2b7-0fd7-417b-c982-834b3c8dbd76","execution":{"iopub.status.busy":"2023-11-29T06:02:41.791012Z","iopub.execute_input":"2023-11-29T06:02:41.791503Z","iopub.status.idle":"2023-11-29T06:02:41.830405Z","shell.execute_reply.started":"2023-11-29T06:02:41.791478Z","shell.execute_reply":"2023-11-29T06:02:41.829415Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"11041\n","output_type":"stream"}]},{"cell_type":"code","source":"def textEncoder(sample):\n    output = list()\n    for i in sample:\n        if i in all_words_vocab.keys():\n            output.append(all_words_vocab[i])\n        else:\n            output.append(all_words_vocab['unk'])\n    return output","metadata":{"id":"_Ra5NpUPE96_","execution":{"iopub.status.busy":"2023-11-29T06:02:41.831488Z","iopub.execute_input":"2023-11-29T06:02:41.831809Z","iopub.status.idle":"2023-11-29T06:02:41.837164Z","shell.execute_reply.started":"2023-11-29T06:02:41.831777Z","shell.execute_reply":"2023-11-29T06:02:41.836325Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"id":"QAwlQD30E938","outputId":"6ef3f494-8f64-4c31-c84a-8200d8f9a403","execution":{"iopub.status.busy":"2023-11-29T06:02:41.838366Z","iopub.execute_input":"2023-11-29T06:02:41.838652Z","iopub.status.idle":"2023-11-29T06:02:47.605250Z","shell.execute_reply.started":"2023-11-29T06:02:41.838629Z","shell.execute_reply":"2023-11-29T06:02:47.604012Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"--2023-11-29 06:02:42--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2023-11-29 06:02:42--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2023-11-29 06:02:43--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: â€˜glove.6B.zip.1â€™\n\nglove.6B.zip.1        1%[                    ]  13.91M  9.58MB/s               ^C\nArchive:  glove.6B.zip\nreplace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}]},{"cell_type":"code","source":"glove_embeddings_dict = dict()\nembedding_dim = 100\nwith open(\"glove.6B.100d.txt\",encoding=\"utf8\") as lines:\n    for line in lines:\n        line_split = line.split()\n        word = line_split[0]\n        vector = np.asarray(line_split[1:], dtype='float32')\n        glove_embeddings_dict[word] = vector","metadata":{"id":"eRA_entfE92T","execution":{"iopub.status.busy":"2023-11-29T06:02:47.606994Z","iopub.execute_input":"2023-11-29T06:02:47.607324Z","iopub.status.idle":"2023-11-29T06:02:59.266060Z","shell.execute_reply.started":"2023-11-29T06:02:47.607297Z","shell.execute_reply":"2023-11-29T06:02:59.265182Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def textEncoder2(sample):\n    mask_list = list()\n    for i in sample:\n        is_uppercase = False\n        for j in i:\n            if j.isupper():\n                is_uppercase = True\n                break\n        if is_uppercase:\n            mask_list.append(1)\n        else:\n            mask_list.append(0)\n    return mask_list","metadata":{"id":"gSYa8j0rE91S","execution":{"iopub.status.busy":"2023-11-29T06:02:59.267185Z","iopub.execute_input":"2023-11-29T06:02:59.267573Z","iopub.status.idle":"2023-11-29T06:02:59.273025Z","shell.execute_reply.started":"2023-11-29T06:02:59.267547Z","shell.execute_reply":"2023-11-29T06:02:59.272053Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"X_train_mask = X_train.map(textEncoder2)\nX_test_mask = X_test.map(textEncoder2)\nX_val_mask = X_val.map(textEncoder2)","metadata":{"id":"pJQjvwhgisX6","execution":{"iopub.status.busy":"2023-11-29T06:02:59.274385Z","iopub.execute_input":"2023-11-29T06:02:59.275094Z","iopub.status.idle":"2023-11-29T06:02:59.626060Z","shell.execute_reply.started":"2023-11-29T06:02:59.275061Z","shell.execute_reply":"2023-11-29T06:02:59.625129Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.map(textEncoder)\nX_test = X_test.map(textEncoder)\nX_val = X_val.map(textEncoder)","metadata":{"id":"aUy63BJtisUu","execution":{"iopub.status.busy":"2023-11-29T06:02:59.627242Z","iopub.execute_input":"2023-11-29T06:02:59.627525Z","iopub.status.idle":"2023-11-29T06:02:59.945088Z","shell.execute_reply.started":"2023-11-29T06:02:59.627502Z","shell.execute_reply":"2023-11-29T06:02:59.943845Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class createDataset2(Dataset):\n    def __init__(self, X_train, y_train, mask):\n        temp1, temp2, lengths, temp3 = list(), list(), list(), list()\n        count = 0\n        for x, y, m in zip(X_train,y_train,mask):\n            count+=1\n            temp1.append(torch.LongTensor(x))\n            lengths.append(len(x))\n            temp2.append(y)\n            temp3.append(torch.Tensor(m))\n        \n        self.X = pad_sequence(temp1,batch_first=True, padding_value=all_words_vocab['pad'])\n        self.X_packed = pack_padded_sequence(self.X, lengths, batch_first=True, enforce_sorted=False)\n        self.y = torch.tensor(temp2)#pad_sequence(temp2,batch_first=True,padding_value=25)  \n        self.mask = pad_sequence(temp3,batch_first=True, padding_value=0)\n        self.lengths = torch.Tensor(lengths).int()\n        self.X = self.X.to(device)\n        self.y = self.y.to(device)\n        self.mask = self.mask.to(device)\n\n\n    def __getitem__(self, index):\n        return self.X[index], self.y[index], self.lengths[index], self.mask[index]\n\n    def __len__(self):\n        return len(self.X)","metadata":{"id":"S02kGmDHisSX","execution":{"iopub.status.busy":"2023-11-29T06:02:59.946614Z","iopub.execute_input":"2023-11-29T06:02:59.946899Z","iopub.status.idle":"2023-11-29T06:02:59.956794Z","shell.execute_reply.started":"2023-11-29T06:02:59.946875Z","shell.execute_reply":"2023-11-29T06:02:59.955964Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\ntrain_data2 = createDataset2(X_train,y_train,X_train_mask)\ntrain_loader2 = torch.utils.data.DataLoader(train_data2,batch_size=batch_size)\nvalidation_data2 = createDataset2(X_val,y_val, X_val_mask)\nvalidation_loader2 = torch.utils.data.DataLoader(validation_data2,batch_size=batch_size)\nvalidation_data2_ = createDataset2(X_val,y_val,X_val_mask)\nvalidation_loader2_ = torch.utils.data.DataLoader(validation_data2_,batch_size=1)\ntrain_data2_ = createDataset2(X_train,y_train,X_train_mask)\ntrain_loader2_ = torch.utils.data.DataLoader(train_data2,batch_size=1)","metadata":{"id":"K_Q4c0uuisRk","execution":{"iopub.status.busy":"2023-11-29T06:16:50.426745Z","iopub.execute_input":"2023-11-29T06:16:50.427104Z","iopub.status.idle":"2023-11-29T06:16:50.907877Z","shell.execute_reply.started":"2023-11-29T06:16:50.427077Z","shell.execute_reply":"2023-11-29T06:16:50.906939Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"train_data21 = createDataset2(X_train,y_train1,X_train_mask)\ntrain_loader21 = torch.utils.data.DataLoader(train_data2,batch_size=batch_size)\nvalidation_data21 = createDataset2(X_val,y_val1, X_val_mask)\nvalidation_loader21 = torch.utils.data.DataLoader(validation_data2,batch_size=batch_size)\nvalidation_data21_ = createDataset2(X_val,y_val1,X_val_mask)\nvalidation_loader21_ = torch.utils.data.DataLoader(validation_data21_,batch_size=1)\ntrain_data21_ = createDataset2(X_train,y_train1,X_train_mask)\ntrain_loader21_ = torch.utils.data.DataLoader(train_data21,batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:00:44.986794Z","iopub.execute_input":"2023-11-29T07:00:44.987714Z","iopub.status.idle":"2023-11-29T07:00:45.455046Z","shell.execute_reply.started":"2023-11-29T07:00:44.987678Z","shell.execute_reply":"2023-11-29T07:00:45.454137Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"class BiLSTMwithGlove(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, lstm_hidden_units, output_dim, number_of_labels, embedding_weights):\n        super(BiLSTMwithGlove, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.embedding.weight.data.copy_(torch.from_numpy(embedding_weights))\n        self.bilstm = nn.LSTM(embedding_dim+1, lstm_hidden_units, bidirectional = True, batch_first = True)\n        self.linear = nn.Linear(lstm_hidden_units * 2 , output_dim)\n        self.dropout = nn.Dropout(0.33)\n        self.relu = nn.ReLU()\n        self.linear1 = nn.Linear(output_dim, number_of_labels)\n\n    def forward(self, x, lengths, mask):\n        x = self.embedding(x)\n        mask = torch.unsqueeze(mask, dim=2)       \n        x = torch.cat((x,mask),dim=2)\n        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n        x, _ = self.bilstm(x)\n        x, _ = pad_packed_sequence(x, batch_first=True)\n        x = self.dropout(x)\n        x = self.linear(x)\n        x = self.relu(x)\n        x = self.linear1(x)\n        x, _ = torch.max(x, dim=1)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:03:00.479784Z","iopub.execute_input":"2023-11-29T06:03:00.480071Z","iopub.status.idle":"2023-11-29T06:03:00.490135Z","shell.execute_reply.started":"2023-11-29T06:03:00.480047Z","shell.execute_reply":"2023-11-29T06:03:00.489175Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(all_words_vocab)\npretrained_weights = np.zeros((vocab_size, embedding_dim), dtype=\"float32\")\nnp.random.seed(69)\nfor word,key in all_words_vocab.items():\n    if key == 0:\n        pretrained_weights[key] = np.zeros(embedding_dim, dtype='float32')\n\n    if key==1:\n        weight = np.random.uniform(-0.01,0.01, embedding_dim)\n        pretrained_weights[key] = weight\n\n    else:\n        lowercase = word.lower()\n        if lowercase in glove_embeddings_dict.keys():\n            if word in glove_embeddings_dict.keys():\n                weight = glove_embeddings_dict[word]\n            else:\n                weight = glove_embeddings_dict[lowercase]\n            pretrained_weights[key] = weight\n        else:\n            temp = np.random.uniform(-0.01,0.01, embedding_dim)\n            pretrained_weights[key] = weight","metadata":{"id":"Lq9eROLnisAC","execution":{"iopub.status.busy":"2023-11-29T06:03:00.491125Z","iopub.execute_input":"2023-11-29T06:03:00.491422Z","iopub.status.idle":"2023-11-29T06:03:00.562323Z","shell.execute_reply.started":"2023-11-29T06:03:00.491384Z","shell.execute_reply":"2023-11-29T06:03:00.561593Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(all_words_vocab)\nembedding_dim = 100\nlstm_hidden_units = 100\nnumber_of_labels = 24 \noutput_dim = 50\nvocab_size","metadata":{"id":"Jb-a6O87ir9O","outputId":"67536fd1-e2a7-41aa-8cf8-96079981b628","execution":{"iopub.status.busy":"2023-11-29T06:03:00.563413Z","iopub.execute_input":"2023-11-29T06:03:00.563709Z","iopub.status.idle":"2023-11-29T06:03:00.570131Z","shell.execute_reply.started":"2023-11-29T06:03:00.563686Z","shell.execute_reply":"2023-11-29T06:03:00.569265Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"11041"},"metadata":{}}]},{"cell_type":"code","source":"model2 = BiLSTMwithGlove(vocab_size, embedding_dim, lstm_hidden_units, output_dim, number_of_labels, pretrained_weights)\ncriterion2 = nn.CrossEntropyLoss()\noptimizer2 = torch.optim.SGD(model2.parameters(),lr=0.4,momentum=0.9)","metadata":{"id":"TXdSih5kqDG-","execution":{"iopub.status.busy":"2023-11-29T06:16:59.818522Z","iopub.execute_input":"2023-11-29T06:16:59.818928Z","iopub.status.idle":"2023-11-29T06:16:59.834888Z","shell.execute_reply.started":"2023-11-29T06:16:59.818900Z","shell.execute_reply":"2023-11-29T06:16:59.833970Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"model2 = model2.to(device)\ncriterion2 = criterion2.to(device)","metadata":{"id":"gQo25q_DqEts","execution":{"iopub.status.busy":"2023-11-29T06:17:01.473852Z","iopub.execute_input":"2023-11-29T06:17:01.474216Z","iopub.status.idle":"2023-11-29T06:17:01.481325Z","shell.execute_reply.started":"2023-11-29T06:17:01.474187Z","shell.execute_reply":"2023-11-29T06:17:01.480312Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def run_train2(model2, optimizer2, loader2, criterion2, flag2):\n    train_loss_epoch = 0\n    train_accuracy_epoch = 0\n    train_loss = list()\n    correct_predictions = 0\n    total_samples = 0\n    for data, ground_truth, sentence_length, mask in loader2:\n        optimizer2.zero_grad()\n        predicted_y = model2(data, sentence_length, mask)\n        training_loss = criterion2(predicted_y, ground_truth)\n        if flag2:\n            training_loss.backward()\n            optimizer2.step()\n        train_loss_epoch += training_loss.item()\n        train_loss.append(train_loss_epoch)\n        _, predicted_labels = torch.max(predicted_y, 1)\n        correct_predictions += (predicted_labels == ground_truth).sum().item()\n        total_samples += ground_truth.size(0)\n        accuracy = correct_predictions / total_samples\n        train_accuracy_epoch += accuracy\n    return train_loss_epoch, train_accuracy_epoch, train_loss\n","metadata":{"id":"c_xgSdWFqI1G","execution":{"iopub.status.busy":"2023-11-29T06:03:00.614324Z","iopub.execute_input":"2023-11-29T06:03:00.614664Z","iopub.status.idle":"2023-11-29T06:03:00.622058Z","shell.execute_reply.started":"2023-11-29T06:03:00.614640Z","shell.execute_reply":"2023-11-29T06:03:00.621325Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def get_prediction_list2(model, loader, sentences, all_emotions):\n    prediction_list = list()\n    ground_truth_list = list()\n    \n    with torch.no_grad():\n        prev = list()\n        for ((x, y, length,mask),sentence) in zip(loader, sentences):\n            prediction = model(x,length,mask)\n            x = x.tolist()\n            y = y.tolist()[0]\n            prediction = prediction.argmax(-1)\n            prediction = prediction.tolist()[0]\n            prediction_list.append(prediction)\n            ground_truth_list.append(y)\n\n    return prediction_list, ground_truth_list","metadata":{"id":"JkLWHCH0qIz-","execution":{"iopub.status.busy":"2023-11-29T06:03:42.928848Z","iopub.execute_input":"2023-11-29T06:03:42.929523Z","iopub.status.idle":"2023-11-29T06:03:42.936385Z","shell.execute_reply.started":"2023-11-29T06:03:42.929489Z","shell.execute_reply":"2023-11-29T06:03:42.935486Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:03:00.635137Z","iopub.execute_input":"2023-11-29T06:03:00.635501Z","iopub.status.idle":"2023-11-29T06:03:00.646416Z","shell.execute_reply.started":"2023-11-29T06:03:00.635470Z","shell.execute_reply":"2023-11-29T06:03:00.645726Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"minimum_training_loss = np.inf\ntrain_loss, validation_loss = list(), list()\nprev_f1 = 0\n\nfor epoch in range(500):\n    model2.train()\n    train_loss_epoch, train_accuracy_epoch, train_loss = run_train2(model2, optimizer2, train_loader2, criterion2, True)\n    train_accuracy_epoch = train_accuracy_epoch / len(train_loader2)\n    train_loss_epoch = train_loss_epoch / len(train_loader2)\n    model2.eval()\n    validation_loss_epoch, validation_accuracy_epoch, validation_loss = run_train2(model2, optimizer2, validation_loader2, criterion2, False)\n    validation_accuracy_epoch = validation_accuracy_epoch / len(validation_loader2)\n    validation_loss_epoch = validation_loss_epoch / len(validation_loader2)\n    print(\"Epoch: {:.0f} Training Accuracy: {:.4f} Training Loss: {:.4f} Validation Accuracy: {:.4f} Validation Loss: {:.4f} \".format(epoch,train_accuracy_epoch,train_loss_epoch,validation_accuracy_epoch,validation_loss_epoch))\n\n    prediction_list, ground_truth_list = get_prediction_list2(model2, validation_loader2_,X_val,idx_to_emotions)\n#     print(prediction_list, ground_truth_list)\n    precision, recall, f1,_ = precision_recall_fscore_support (ground_truth_list, prediction_list, average='weighted',zero_division=0)\n    print(precision, recall,  f1)","metadata":{"id":"0ia92t69qIhw","outputId":"80b495d2-584e-4450-9ef0-70ff9407005c","execution":{"iopub.status.busy":"2023-11-29T06:17:05.209033Z","iopub.execute_input":"2023-11-29T06:17:05.209408Z","iopub.status.idle":"2023-11-29T06:39:28.411710Z","shell.execute_reply.started":"2023-11-29T06:17:05.209379Z","shell.execute_reply":"2023-11-29T06:39:28.410344Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Epoch: 0 Training Accuracy: 0.0376 Training Loss: 3.1779 Validation Accuracy: 0.0229 Validation Loss: 3.2060 \n0.0005012252171975941 0.022388059701492536 0.0009804989650288702\nEpoch: 1 Training Accuracy: 0.0576 Training Loss: 3.1203 Validation Accuracy: 0.0229 Validation Loss: 3.2898 \n0.0005049938278532151 0.022388059701492536 0.000987708516242318\nEpoch: 2 Training Accuracy: 0.0541 Training Loss: 3.0976 Validation Accuracy: 0.0458 Validation Loss: 3.3766 \n0.0020049008687903764 0.04477611940298507 0.0038379530916844346\nEpoch: 3 Training Accuracy: 0.0448 Training Loss: 3.1219 Validation Accuracy: 0.0458 Validation Loss: 3.3086 \n0.0020049008687903764 0.04477611940298507 0.0038379530916844346\nEpoch: 4 Training Accuracy: 0.0322 Training Loss: 3.1106 Validation Accuracy: 0.0458 Validation Loss: 3.2953 \n0.0020049008687903764 0.04477611940298507 0.0038379530916844346\nEpoch: 5 Training Accuracy: 0.0470 Training Loss: 3.0955 Validation Accuracy: 0.0458 Validation Loss: 3.3221 \n0.0020049008687903764 0.04477611940298507 0.0038379530916844346\nEpoch: 6 Training Accuracy: 0.0476 Training Loss: 3.0906 Validation Accuracy: 0.0458 Validation Loss: 3.3418 \n0.0033134128151996556 0.04477611940298507 0.006162249423554838\nEpoch: 7 Training Accuracy: 0.0389 Training Loss: 3.0899 Validation Accuracy: 0.0076 Validation Loss: 3.3393 \n0.0003827018752391887 0.007462686567164179 0.0007280669821623589\nEpoch: 8 Training Accuracy: 0.0542 Training Loss: 3.0864 Validation Accuracy: 0.0458 Validation Loss: 3.3317 \n0.0020049008687903764 0.04477611940298507 0.0038379530916844346\nEpoch: 9 Training Accuracy: 0.0470 Training Loss: 3.0855 Validation Accuracy: 0.0458 Validation Loss: 3.3199 \n0.0020049008687903764 0.04477611940298507 0.0038379530916844346\nEpoch: 10 Training Accuracy: 0.0470 Training Loss: 3.0823 Validation Accuracy: 0.0076 Validation Loss: 3.3097 \n0.0002763957987838585 0.007462686567164179 0.0005330490405117271\nEpoch: 11 Training Accuracy: 0.0631 Training Loss: 3.0688 Validation Accuracy: 0.0229 Validation Loss: 3.3087 \n0.001966060681915371 0.022388059701492536 0.0035687181525785117\nEpoch: 12 Training Accuracy: 0.0592 Training Loss: 3.0616 Validation Accuracy: 0.0306 Validation Loss: 3.3091 \n0.002322712572654227 0.029850746268656716 0.004234284290996877\nEpoch: 13 Training Accuracy: 0.0731 Training Loss: 3.0486 Validation Accuracy: 0.0382 Validation Loss: 3.3043 \n0.002848026724340992 0.03731343283582089 0.005224794022543342\nEpoch: 14 Training Accuracy: 0.0772 Training Loss: 3.0436 Validation Accuracy: 0.0306 Validation Loss: 3.2809 \n0.0027419084353513335 0.029850746268656716 0.0048669695003244655\nEpoch: 15 Training Accuracy: 0.0715 Training Loss: 3.0214 Validation Accuracy: 0.0306 Validation Loss: 3.2515 \n0.0025499461937959107 0.029850746268656716 0.004577114427860696\nEpoch: 16 Training Accuracy: 0.0741 Training Loss: 3.0247 Validation Accuracy: 0.0458 Validation Loss: 3.2358 \n0.0035940697848549892 0.04477611940298507 0.006467661691542289\nEpoch: 17 Training Accuracy: 0.0779 Training Loss: 3.0117 Validation Accuracy: 0.0458 Validation Loss: 3.2427 \n0.004070556309362279 0.04477611940298507 0.0071881395697099785\nEpoch: 18 Training Accuracy: 0.0667 Training Loss: 2.9741 Validation Accuracy: 0.0382 Validation Loss: 3.2408 \n0.00455758453505567 0.03731343283582089 0.007987713130477568\nEpoch: 19 Training Accuracy: 0.0679 Training Loss: 2.9499 Validation Accuracy: 0.0535 Validation Loss: 3.2354 \n0.005178014849352558 0.05223880597014925 0.009287975705886154\nEpoch: 20 Training Accuracy: 0.0739 Training Loss: 2.9546 Validation Accuracy: 0.0611 Validation Loss: 3.2510 \n0.006515043828476665 0.05970149253731343 0.011368220323444204\nEpoch: 21 Training Accuracy: 0.0764 Training Loss: 2.9181 Validation Accuracy: 0.0458 Validation Loss: 3.2461 \n0.005121248766826847 0.04477611940298507 0.008942113602817355\nEpoch: 22 Training Accuracy: 0.0587 Training Loss: 2.9245 Validation Accuracy: 0.0535 Validation Loss: 3.2251 \n0.019928582891991654 0.05223880597014925 0.018737523401702504\nEpoch: 23 Training Accuracy: 0.0784 Training Loss: 2.9119 Validation Accuracy: 0.0306 Validation Loss: 3.2261 \n0.0028142589118198874 0.029850746268656716 0.0048811176674765705\nEpoch: 24 Training Accuracy: 0.1061 Training Loss: 2.8708 Validation Accuracy: 0.0382 Validation Loss: 3.3532 \n0.003617317141003059 0.03731343283582089 0.006556698454353038\nEpoch: 25 Training Accuracy: 0.0668 Training Loss: 2.9338 Validation Accuracy: 0.0458 Validation Loss: 3.2742 \n0.006786183764509711 0.04477611940298507 0.010618571311359246\nEpoch: 26 Training Accuracy: 0.0723 Training Loss: 2.8581 Validation Accuracy: 0.0382 Validation Loss: 3.3079 \n0.003924080822198927 0.03731343283582089 0.0067647464946085385\nEpoch: 27 Training Accuracy: 0.0923 Training Loss: 2.9093 Validation Accuracy: 0.0840 Validation Loss: 3.3488 \n0.020227808326787115 0.05970149253731343 0.027374635772543926\nEpoch: 28 Training Accuracy: 0.0626 Training Loss: 2.9165 Validation Accuracy: 0.0229 Validation Loss: 3.2900 \n0.005879692446856626 0.022388059701492536 0.007675906183368871\nEpoch: 29 Training Accuracy: 0.0864 Training Loss: 2.8468 Validation Accuracy: 0.0611 Validation Loss: 3.2910 \n0.014269097187063326 0.05970149253731343 0.018940508723985706\nEpoch: 30 Training Accuracy: 0.0762 Training Loss: 2.8388 Validation Accuracy: 0.0382 Validation Loss: 3.3753 \n0.018634882601247328 0.03731343283582089 0.019617819990954317\nEpoch: 31 Training Accuracy: 0.0855 Training Loss: 2.8027 Validation Accuracy: 0.0764 Validation Loss: 3.3164 \n0.012460162953349195 0.04477611940298507 0.016015059725081045\nEpoch: 32 Training Accuracy: 0.0949 Training Loss: 2.8052 Validation Accuracy: 0.0535 Validation Loss: 3.4282 \n0.043891652846876725 0.05223880597014925 0.03355700489150884\nEpoch: 33 Training Accuracy: 0.0993 Training Loss: 2.7482 Validation Accuracy: 0.0687 Validation Loss: 3.3209 \n0.04071185867606102 0.06716417910447761 0.029611146105894316\nEpoch: 34 Training Accuracy: 0.0851 Training Loss: 2.7178 Validation Accuracy: 0.0764 Validation Loss: 3.5424 \n0.03775310378200184 0.05223880597014925 0.03316108887268542\nEpoch: 35 Training Accuracy: 0.0935 Training Loss: 2.7342 Validation Accuracy: 0.0611 Validation Loss: 3.3768 \n0.05541231299905957 0.05970149253731343 0.03540891346093344\nEpoch: 36 Training Accuracy: 0.1179 Training Loss: 2.6529 Validation Accuracy: 0.0917 Validation Loss: 3.5416 \n0.03710021321961621 0.05223880597014925 0.03139119548352454\nEpoch: 37 Training Accuracy: 0.1187 Training Loss: 2.6398 Validation Accuracy: 0.0458 Validation Loss: 3.5649 \n0.02785040537454936 0.04477611940298507 0.02056768941217665\nEpoch: 38 Training Accuracy: 0.0993 Training Loss: 2.7397 Validation Accuracy: 0.0687 Validation Loss: 3.6338 \n0.012067822201365012 0.05970149253731343 0.019864314789687924\nEpoch: 39 Training Accuracy: 0.1154 Training Loss: 2.6730 Validation Accuracy: 0.0687 Validation Loss: 3.5429 \n0.023155752338784544 0.06716417910447761 0.029750457200165184\nEpoch: 40 Training Accuracy: 0.1106 Training Loss: 2.5721 Validation Accuracy: 0.0687 Validation Loss: 3.8874 \n0.010361398985140742 0.05970149253731343 0.01654887846956291\nEpoch: 41 Training Accuracy: 0.1095 Training Loss: 2.6959 Validation Accuracy: 0.0687 Validation Loss: 3.5676 \n0.021415478995997456 0.03731343283582089 0.022462686567164176\nEpoch: 42 Training Accuracy: 0.1516 Training Loss: 2.6091 Validation Accuracy: 0.0458 Validation Loss: 3.7464 \n0.0050869421366076025 0.03731343283582089 0.008765695332859513\nEpoch: 43 Training Accuracy: 0.1090 Training Loss: 2.6736 Validation Accuracy: 0.0764 Validation Loss: 3.5122 \n0.03258116098102214 0.05223880597014925 0.03105712706759278\nEpoch: 44 Training Accuracy: 0.1361 Training Loss: 2.6829 Validation Accuracy: 0.0535 Validation Loss: 3.4134 \n0.0032649253731343282 0.029850746268656716 0.005618964003511852\nEpoch: 45 Training Accuracy: 0.1203 Training Loss: 2.6150 Validation Accuracy: 0.0611 Validation Loss: 3.5962 \n0.027120038903228217 0.029850746268656716 0.015506384163100578\nEpoch: 46 Training Accuracy: 0.1353 Training Loss: 2.5420 Validation Accuracy: 0.0535 Validation Loss: 3.5983 \n0.011165762098597919 0.03731343283582089 0.015587397676949916\nEpoch: 47 Training Accuracy: 0.1541 Training Loss: 2.4681 Validation Accuracy: 0.0687 Validation Loss: 3.7784 \n0.040828325902952774 0.05970149253731343 0.04213484176170743\nEpoch: 48 Training Accuracy: 0.1783 Training Loss: 2.4160 Validation Accuracy: 0.0535 Validation Loss: 3.7384 \n0.014001421464108032 0.03731343283582089 0.017723880597014924\nEpoch: 49 Training Accuracy: 0.1720 Training Loss: 2.3799 Validation Accuracy: 0.0687 Validation Loss: 3.8950 \n0.03601862482459497 0.04477611940298507 0.02784068811815474\nEpoch: 50 Training Accuracy: 0.1537 Training Loss: 2.4311 Validation Accuracy: 0.0840 Validation Loss: 3.6651 \n0.060840817557235465 0.05970149253731343 0.038541396376062055\nEpoch: 51 Training Accuracy: 0.1728 Training Loss: 2.3415 Validation Accuracy: 0.0611 Validation Loss: 3.9526 \n0.02291427477994642 0.04477611940298507 0.0238272921108742\nEpoch: 52 Training Accuracy: 0.1659 Training Loss: 2.3202 Validation Accuracy: 0.0458 Validation Loss: 3.7063 \n0.011593547414442937 0.029850746268656716 0.014857033513749932\nEpoch: 53 Training Accuracy: 0.2153 Training Loss: 2.2516 Validation Accuracy: 0.0840 Validation Loss: 4.1990 \n0.10060106167725963 0.07462686567164178 0.068217888345193\nEpoch: 54 Training Accuracy: 0.2378 Training Loss: 2.2174 Validation Accuracy: 0.0687 Validation Loss: 3.9357 \n0.05830223880597015 0.03731343283582089 0.03406046689628779\nEpoch: 55 Training Accuracy: 0.2166 Training Loss: 2.2257 Validation Accuracy: 0.0306 Validation Loss: 4.1766 \n0.03950918484500574 0.05223880597014925 0.04004405557014031\nEpoch: 56 Training Accuracy: 0.2483 Training Loss: 2.1549 Validation Accuracy: 0.0306 Validation Loss: 3.9581 \n0.009294036532842501 0.029850746268656716 0.01382542109298818\nEpoch: 57 Training Accuracy: 0.1920 Training Loss: 2.3264 Validation Accuracy: 0.0382 Validation Loss: 4.3543 \n0.10781401434386509 0.05970149253731343 0.046398366687498284\nEpoch: 58 Training Accuracy: 0.2340 Training Loss: 2.2590 Validation Accuracy: 0.0306 Validation Loss: 4.0249 \n0.007105620224237663 0.022388059701492536 0.010787442099845598\nEpoch: 59 Training Accuracy: 0.2569 Training Loss: 2.1314 Validation Accuracy: 0.0382 Validation Loss: 3.9305 \n0.035980810234541576 0.03731343283582089 0.03144565337162721\nEpoch: 60 Training Accuracy: 0.2443 Training Loss: 2.1639 Validation Accuracy: 0.0458 Validation Loss: 3.8250 \n0.02702017861005531 0.05970149253731343 0.036584124830393486\nEpoch: 61 Training Accuracy: 0.2819 Training Loss: 2.0492 Validation Accuracy: 0.0535 Validation Loss: 4.1152 \n0.051639604176917604 0.05223880597014925 0.0419299713913601\nEpoch: 62 Training Accuracy: 0.2811 Training Loss: 2.0756 Validation Accuracy: 0.0382 Validation Loss: 4.1294 \n0.07153518123667377 0.05970149253731343 0.04773932535126565\nEpoch: 63 Training Accuracy: 0.3150 Training Loss: 2.0852 Validation Accuracy: 0.0382 Validation Loss: 4.2008 \n0.044791432713314414 0.029850746268656716 0.019047072330654426\nEpoch: 64 Training Accuracy: 0.3149 Training Loss: 1.9458 Validation Accuracy: 0.0306 Validation Loss: 4.2712 \n0.05708885544267458 0.04477611940298507 0.03427433494424122\nEpoch: 65 Training Accuracy: 0.3072 Training Loss: 2.0635 Validation Accuracy: 0.0076 Validation Loss: 4.3333 \n0.012859808102345415 0.029850746268656716 0.017004140138468498\nEpoch: 66 Training Accuracy: 0.2684 Training Loss: 2.0834 Validation Accuracy: 0.0382 Validation Loss: 4.1914 \n0.07199004975124378 0.05223880597014925 0.040116359326758944\nEpoch: 67 Training Accuracy: 0.2780 Training Loss: 2.1052 Validation Accuracy: 0.0306 Validation Loss: 4.3909 \n0.10312075983717775 0.029850746268656716 0.028439734102596266\nEpoch: 68 Training Accuracy: 0.3006 Training Loss: 2.0509 Validation Accuracy: 0.0076 Validation Loss: 4.0659 \n0.008039571877454832 0.029850746268656716 0.012459072160564697\nEpoch: 69 Training Accuracy: 0.2745 Training Loss: 1.9846 Validation Accuracy: 0.0229 Validation Loss: 4.2780 \n0.04161838323908408 0.05223880597014925 0.0383172578080392\nEpoch: 70 Training Accuracy: 0.3055 Training Loss: 1.9266 Validation Accuracy: 0.0458 Validation Loss: 4.0786 \n0.0498030679933665 0.06716417910447761 0.042992828067454934\nEpoch: 71 Training Accuracy: 0.3974 Training Loss: 1.8099 Validation Accuracy: 0.0382 Validation Loss: 4.3311 \n0.059622901973648244 0.06716417910447761 0.05253872003992369\nEpoch: 72 Training Accuracy: 0.3538 Training Loss: 1.7554 Validation Accuracy: 0.0229 Validation Loss: 4.1667 \n0.004757462686567164 0.014925373134328358 0.007030067056024226\nEpoch: 73 Training Accuracy: 0.3785 Training Loss: 1.6981 Validation Accuracy: 0.0306 Validation Loss: 4.2400 \n0.07125977256574272 0.04477611940298507 0.037331412714943966\nEpoch: 74 Training Accuracy: 0.4008 Training Loss: 1.7099 Validation Accuracy: 0.0153 Validation Loss: 4.2601 \n0.018656716417910446 0.022388059701492536 0.020168388825105243\nEpoch: 75 Training Accuracy: 0.4211 Training Loss: 1.6355 Validation Accuracy: 0.0076 Validation Loss: 4.4110 \n0.013514004652064354 0.022388059701492536 0.01568286387610818\nEpoch: 76 Training Accuracy: 0.4439 Training Loss: 1.5881 Validation Accuracy: 0.0229 Validation Loss: 4.5855 \n0.01007947276603993 0.022388059701492536 0.012985074626865671\nEpoch: 77 Training Accuracy: 0.4403 Training Loss: 1.6108 Validation Accuracy: 0.0306 Validation Loss: 4.7406 \n0.01692701991209454 0.03731343283582089 0.022876004592422505\nEpoch: 78 Training Accuracy: 0.4361 Training Loss: 1.5771 Validation Accuracy: 0.0306 Validation Loss: 4.6123 \n0.02763157894736842 0.03731343283582089 0.022174840085287847\nEpoch: 79 Training Accuracy: 0.4695 Training Loss: 1.6098 Validation Accuracy: 0.0229 Validation Loss: 5.0052 \n0.02303620705034688 0.05223880597014925 0.031529196124639954\nEpoch: 80 Training Accuracy: 0.4335 Training Loss: 1.6794 Validation Accuracy: 0.0382 Validation Loss: 5.0599 \n0.04548846675712347 0.05223880597014925 0.040888525593802454\nEpoch: 81 Training Accuracy: 0.4005 Training Loss: 1.8460 Validation Accuracy: 0.0306 Validation Loss: 4.6745 \n0.06896685404148091 0.04477611940298507 0.04235771211419287\nEpoch: 82 Training Accuracy: 0.4179 Training Loss: 1.8226 Validation Accuracy: 0.0306 Validation Loss: 4.3653 \n0.04423014796149124 0.03731343283582089 0.03342773312922566\nEpoch: 83 Training Accuracy: 0.3986 Training Loss: 1.8682 Validation Accuracy: 0.0458 Validation Loss: 4.4584 \n0.09886282871357498 0.05970149253731343 0.059079601990049746\nEpoch: 84 Training Accuracy: 0.3789 Training Loss: 1.9008 Validation Accuracy: 0.0229 Validation Loss: 4.4925 \n0.01201551922087177 0.03731343283582089 0.018106582472254117\nEpoch: 85 Training Accuracy: 0.3538 Training Loss: 1.9104 Validation Accuracy: 0.0306 Validation Loss: 5.1294 \n0.012748756218905472 0.022388059701492536 0.013242060374817642\nEpoch: 86 Training Accuracy: 0.3690 Training Loss: 1.8735 Validation Accuracy: 0.0611 Validation Loss: 4.2852 \n0.02487562189054726 0.022388059701492536 0.023217247097844118\nEpoch: 87 Training Accuracy: 0.4074 Training Loss: 1.7215 Validation Accuracy: 0.0764 Validation Loss: 4.8514 \n0.03284679543459175 0.04477611940298507 0.031170809155883782\nEpoch: 88 Training Accuracy: 0.3764 Training Loss: 1.7598 Validation Accuracy: 0.0458 Validation Loss: 4.5236 \n0.03536529916715891 0.05970149253731343 0.03644183176190658\nEpoch: 89 Training Accuracy: 0.3763 Training Loss: 1.6400 Validation Accuracy: 0.0229 Validation Loss: 5.0555 \n0.05431769722814499 0.04477611940298507 0.04002890812694394\nEpoch: 90 Training Accuracy: 0.4716 Training Loss: 1.5212 Validation Accuracy: 0.0306 Validation Loss: 4.8204 \n0.029953493402552454 0.04477611940298507 0.029338185100725778\nEpoch: 91 Training Accuracy: 0.4434 Training Loss: 1.5322 Validation Accuracy: 0.0229 Validation Loss: 4.9313 \n0.026990049751243785 0.029850746268656716 0.023349598163031\nEpoch: 92 Training Accuracy: 0.4750 Training Loss: 1.5185 Validation Accuracy: 0.0306 Validation Loss: 5.3343 \n0.03582089552238806 0.029850746268656716 0.022716089880268988\nEpoch: 93 Training Accuracy: 0.4942 Training Loss: 1.4994 Validation Accuracy: 0.0458 Validation Loss: 5.2102 \n0.05660980810234542 0.05223880597014925 0.04611466477138119\nEpoch: 94 Training Accuracy: 0.5312 Training Loss: 1.3362 Validation Accuracy: 0.0229 Validation Loss: 5.6051 \n0.028104668962877923 0.04477611940298507 0.033914873791577226\nEpoch: 95 Training Accuracy: 0.5235 Training Loss: 1.4211 Validation Accuracy: 0.0306 Validation Loss: 5.8144 \n0.03558102345415778 0.03731343283582089 0.0321614190656078\nEpoch: 96 Training Accuracy: 0.5219 Training Loss: 1.4032 Validation Accuracy: 0.0306 Validation Loss: 5.5874 \n0.0378300803673938 0.03731343283582089 0.03113934830352741\nEpoch: 97 Training Accuracy: 0.5550 Training Loss: 1.3409 Validation Accuracy: 0.0229 Validation Loss: 5.4655 \n0.02338308457711443 0.022388059701492536 0.0203150912106136\nEpoch: 98 Training Accuracy: 0.5001 Training Loss: 1.3770 Validation Accuracy: 0.1069 Validation Loss: 6.1918 \n0.0769966658188592 0.08208955223880597 0.07080235270533779\nEpoch: 99 Training Accuracy: 0.5418 Training Loss: 1.3778 Validation Accuracy: 0.0917 Validation Loss: 5.6142 \n0.07095443833464259 0.06716417910447761 0.05942689151644376\nEpoch: 100 Training Accuracy: 0.5554 Training Loss: 1.2639 Validation Accuracy: 0.0764 Validation Loss: 5.7634 \n0.027321376311718715 0.029850746268656716 0.027656224372642285\nEpoch: 101 Training Accuracy: 0.5902 Training Loss: 1.2163 Validation Accuracy: 0.0382 Validation Loss: 6.0056 \n0.02487562189054726 0.029850746268656716 0.026628760957119163\nEpoch: 102 Training Accuracy: 0.5776 Training Loss: 1.2095 Validation Accuracy: 0.0458 Validation Loss: 6.1120 \n0.04602685871342587 0.05970149253731343 0.04395634608910336\nEpoch: 103 Training Accuracy: 0.5695 Training Loss: 1.2625 Validation Accuracy: 0.0382 Validation Loss: 6.2086 \n0.037926132882386096 0.05223880597014925 0.03884424033677765\nEpoch: 104 Training Accuracy: 0.5811 Training Loss: 1.2279 Validation Accuracy: 0.0458 Validation Loss: 6.0085 \n0.04925373134328358 0.03731343283582089 0.04092039800995025\nEpoch: 105 Training Accuracy: 0.5899 Training Loss: 1.2082 Validation Accuracy: 0.0306 Validation Loss: 6.1648 \n0.03437358661239258 0.04477611940298507 0.03560767590618337\nEpoch: 106 Training Accuracy: 0.5887 Training Loss: 1.1932 Validation Accuracy: 0.0764 Validation Loss: 6.2406 \n0.02425045823514009 0.03731343283582089 0.02700135685210312\nEpoch: 107 Training Accuracy: 0.5585 Training Loss: 1.2710 Validation Accuracy: 0.0611 Validation Loss: 6.0399 \n0.04394693200663349 0.03731343283582089 0.04029850746268656\nEpoch: 108 Training Accuracy: 0.6083 Training Loss: 1.1738 Validation Accuracy: 0.0611 Validation Loss: 6.2943 \n0.03450604122245913 0.03731343283582089 0.03462686567164179\nEpoch: 109 Training Accuracy: 0.5959 Training Loss: 1.1628 Validation Accuracy: 0.0382 Validation Loss: 6.6964 \n0.060548099836949705 0.04477611940298507 0.04673892442549158\nEpoch: 110 Training Accuracy: 0.6111 Training Loss: 1.1382 Validation Accuracy: 0.0382 Validation Loss: 6.5954 \n0.038272921108742 0.03731343283582089 0.033762862705108004\nEpoch: 111 Training Accuracy: 0.6044 Training Loss: 1.1843 Validation Accuracy: 0.0382 Validation Loss: 6.0506 \n0.07000872262066292 0.03731343283582089 0.03338042067190443\nEpoch: 112 Training Accuracy: 0.6047 Training Loss: 1.2576 Validation Accuracy: 0.0535 Validation Loss: 6.1774 \n0.08470149253731343 0.04477611940298507 0.0449816190731181\nEpoch: 113 Training Accuracy: 0.5770 Training Loss: 1.2871 Validation Accuracy: 0.0382 Validation Loss: 5.9722 \n0.04735252309879175 0.029850746268656716 0.033665008291873966\nEpoch: 114 Training Accuracy: 0.6077 Training Loss: 1.1931 Validation Accuracy: 0.0458 Validation Loss: 6.3707 \n0.06638237384506042 0.03731343283582089 0.04005294042607476\nEpoch: 115 Training Accuracy: 0.5772 Training Loss: 1.2293 Validation Accuracy: 0.0229 Validation Loss: 7.0048 \n0.03059701492537313 0.022388059701492536 0.02306648575305292\nEpoch: 116 Training Accuracy: 0.6294 Training Loss: 1.1702 Validation Accuracy: 0.0306 Validation Loss: 7.0611 \n0.04495380241648898 0.022388059701492536 0.027825615512182676\nEpoch: 117 Training Accuracy: 0.6412 Training Loss: 1.1945 Validation Accuracy: 0.0535 Validation Loss: 6.4287 \n0.08328056686265642 0.07462686567164178 0.06337335355114108\nEpoch: 118 Training Accuracy: 0.6008 Training Loss: 1.1884 Validation Accuracy: 0.0306 Validation Loss: 6.6855 \n0.014803055117785811 0.029850746268656716 0.01777634130575307\nEpoch: 119 Training Accuracy: 0.6148 Training Loss: 1.1339 Validation Accuracy: 0.0229 Validation Loss: 6.7126 \n0.01274548311076198 0.022388059701492536 0.015065365811634467\nEpoch: 120 Training Accuracy: 0.6195 Training Loss: 1.1505 Validation Accuracy: 0.0611 Validation Loss: 5.9268 \n0.061426633068424115 0.05970149253731343 0.05773887190413892\nEpoch: 121 Training Accuracy: 0.6692 Training Loss: 1.0625 Validation Accuracy: 0.0382 Validation Loss: 6.5907 \n0.053861332435566525 0.04477611940298507 0.03951454846977235\nEpoch: 122 Training Accuracy: 0.6363 Training Loss: 1.0949 Validation Accuracy: 0.0382 Validation Loss: 7.0010 \n0.06604477611940299 0.03731343283582089 0.030177885061077787\nEpoch: 123 Training Accuracy: 0.6428 Training Loss: 1.0626 Validation Accuracy: 0.0306 Validation Loss: 6.9294 \n0.04547728391011974 0.03731343283582089 0.0343798250128667\nEpoch: 124 Training Accuracy: 0.6401 Training Loss: 1.0579 Validation Accuracy: 0.0458 Validation Loss: 7.3644 \n0.05842307032476873 0.05223880597014925 0.04293710021321962\nEpoch: 125 Training Accuracy: 0.6644 Training Loss: 1.0648 Validation Accuracy: 0.0382 Validation Loss: 8.0110 \n0.025126291618828938 0.05223880597014925 0.030357370655878117\nEpoch: 126 Training Accuracy: 0.6286 Training Loss: 1.1043 Validation Accuracy: 0.0458 Validation Loss: 7.4185 \n0.03084126876927579 0.04477611940298507 0.03264925373134328\nEpoch: 127 Training Accuracy: 0.6508 Training Loss: 1.0909 Validation Accuracy: 0.0382 Validation Loss: 7.9505 \n0.028917910447761194 0.04477611940298507 0.02976247793291606\nEpoch: 128 Training Accuracy: 0.6105 Training Loss: 1.1838 Validation Accuracy: 0.0611 Validation Loss: 7.7945 \n0.05093283582089552 0.06716417910447761 0.05102923610386297\nEpoch: 129 Training Accuracy: 0.6308 Training Loss: 1.1047 Validation Accuracy: 0.0993 Validation Loss: 7.4058 \n0.0610187697874265 0.07462686567164178 0.06400736764137153\nEpoch: 130 Training Accuracy: 0.6247 Training Loss: 1.1061 Validation Accuracy: 0.0840 Validation Loss: 7.3231 \n0.043179933665008295 0.04477611940298507 0.04268592758380661\nEpoch: 131 Training Accuracy: 0.6312 Training Loss: 1.1341 Validation Accuracy: 0.0687 Validation Loss: 8.1996 \n0.08149271955242104 0.06716417910447761 0.06960776623333405\nEpoch: 132 Training Accuracy: 0.6489 Training Loss: 1.1044 Validation Accuracy: 0.0458 Validation Loss: 7.9474 \n0.05466498675453899 0.05223880597014925 0.04298964132151139\nEpoch: 133 Training Accuracy: 0.6128 Training Loss: 1.1336 Validation Accuracy: 0.0306 Validation Loss: 7.8634 \n0.04971434038598218 0.05223880597014925 0.04287616012397279\nEpoch: 134 Training Accuracy: 0.6206 Training Loss: 1.3092 Validation Accuracy: 0.0764 Validation Loss: 6.7719 \n0.10575328485776246 0.07462686567164178 0.07163377788926516\nEpoch: 135 Training Accuracy: 0.5950 Training Loss: 1.2144 Validation Accuracy: 0.0764 Validation Loss: 6.3289 \n0.1033345178867567 0.07462686567164178 0.07557279610500349\nEpoch: 136 Training Accuracy: 0.5224 Training Loss: 1.3831 Validation Accuracy: 0.0648 Validation Loss: 6.7924 \n0.06343283582089553 0.06716417910447761 0.05721402570586882\nEpoch: 137 Training Accuracy: 0.5886 Training Loss: 1.2246 Validation Accuracy: 0.0993 Validation Loss: 6.7815 \n0.0881353306726441 0.07462686567164178 0.07663998522207477\nEpoch: 138 Training Accuracy: 0.6091 Training Loss: 1.1630 Validation Accuracy: 0.0917 Validation Loss: 6.6054 \n0.08702025586353945 0.08208955223880597 0.07541974956515157\nEpoch: 139 Training Accuracy: 0.5956 Training Loss: 1.2008 Validation Accuracy: 0.0687 Validation Loss: 6.7601 \n0.03868474448036315 0.04477611940298507 0.03658117216640391\nEpoch: 140 Training Accuracy: 0.5876 Training Loss: 1.1737 Validation Accuracy: 0.0687 Validation Loss: 6.5436 \n0.03751696065128901 0.05223880597014925 0.03485944098669943\nEpoch: 141 Training Accuracy: 0.6663 Training Loss: 1.0892 Validation Accuracy: 0.0764 Validation Loss: 7.8907 \n0.03345997286295794 0.05970149253731343 0.0382502752344719\nEpoch: 142 Training Accuracy: 0.6185 Training Loss: 1.2002 Validation Accuracy: 0.0993 Validation Loss: 6.9678 \n0.04632551528073916 0.06716417910447761 0.0522308330144151\nEpoch: 143 Training Accuracy: 0.6435 Training Loss: 1.0832 Validation Accuracy: 0.0993 Validation Loss: 6.7513 \n0.05555555555555555 0.07462686567164178 0.05764385522830142\nEpoch: 144 Training Accuracy: 0.6294 Training Loss: 1.1953 Validation Accuracy: 0.0993 Validation Loss: 8.1423 \n0.05783582089552239 0.07462686567164178 0.052659177977707285\nEpoch: 145 Training Accuracy: 0.5998 Training Loss: 1.1621 Validation Accuracy: 0.0687 Validation Loss: 7.7494 \n0.07025882492300403 0.08208955223880597 0.06076879270214917\nEpoch: 146 Training Accuracy: 0.6421 Training Loss: 1.1393 Validation Accuracy: 0.0764 Validation Loss: 7.6617 \n0.07566242971823764 0.07462686567164178 0.06740352857560936\nEpoch: 147 Training Accuracy: 0.6461 Training Loss: 1.1462 Validation Accuracy: 0.0764 Validation Loss: 6.9964 \n0.0848303934871099 0.08955223880597014 0.07719914937118393\nEpoch: 148 Training Accuracy: 0.6347 Training Loss: 1.1735 Validation Accuracy: 0.0458 Validation Loss: 7.4884 \n0.036380597014925374 0.05970149253731343 0.0423714759535655\nEpoch: 149 Training Accuracy: 0.6748 Training Loss: 1.1195 Validation Accuracy: 0.0382 Validation Loss: 8.1862 \n0.03057787983161117 0.05970149253731343 0.039370419447010176\nEpoch: 150 Training Accuracy: 0.6787 Training Loss: 1.0565 Validation Accuracy: 0.0382 Validation Loss: 8.4665 \n0.016479507225775878 0.029850746268656716 0.02003860968112292\nEpoch: 151 Training Accuracy: 0.6337 Training Loss: 1.2377 Validation Accuracy: 0.0458 Validation Loss: 8.7245 \n0.047851080730799786 0.05970149253731343 0.04871622483562782\nEpoch: 152 Training Accuracy: 0.6241 Training Loss: 1.1856 Validation Accuracy: 0.0917 Validation Loss: 8.3781 \n0.066181459015085 0.06716417910447761 0.05899261456691697\nEpoch: 153 Training Accuracy: 0.6164 Training Loss: 1.1742 Validation Accuracy: 0.0993 Validation Loss: 7.7135 \n0.08808362035973975 0.07462686567164178 0.07332329013397355\nEpoch: 154 Training Accuracy: 0.6060 Training Loss: 1.2154 Validation Accuracy: 0.1222 Validation Loss: 7.6006 \n0.11459529276693455 0.09701492537313433 0.09612220283862073\nEpoch: 155 Training Accuracy: 0.6350 Training Loss: 1.1251 Validation Accuracy: 0.0840 Validation Loss: 8.0801 \n0.08333768221827924 0.08208955223880597 0.07717609431832435\nEpoch: 156 Training Accuracy: 0.6440 Training Loss: 1.1315 Validation Accuracy: 0.0687 Validation Loss: 8.2556 \n0.04014679350500246 0.06716417910447761 0.04739398246860934\nEpoch: 157 Training Accuracy: 0.6370 Training Loss: 1.1151 Validation Accuracy: 0.0535 Validation Loss: 8.8818 \n0.038496619466768724 0.05223880597014925 0.040068016560553865\nEpoch: 158 Training Accuracy: 0.6380 Training Loss: 1.2308 Validation Accuracy: 0.0764 Validation Loss: 10.2685 \n0.02400497512437811 0.04477611940298507 0.029993389694882234\nEpoch: 159 Training Accuracy: 0.6145 Training Loss: 1.2855 Validation Accuracy: 0.0535 Validation Loss: 9.7815 \n0.011371712864250177 0.022388059701492536 0.014351320321469574\nEpoch: 160 Training Accuracy: 0.6067 Training Loss: 1.3427 Validation Accuracy: 0.0687 Validation Loss: 8.7851 \n0.04023153463451971 0.05223880597014925 0.038780125347289536\nEpoch: 161 Training Accuracy: 0.6160 Training Loss: 1.2170 Validation Accuracy: 0.0993 Validation Loss: 9.4992 \n0.07575111455708472 0.07462686567164178 0.0675060428791772\nEpoch: 162 Training Accuracy: 0.5594 Training Loss: 1.4658 Validation Accuracy: 0.0917 Validation Loss: 9.5795 \n0.08362085675518513 0.07462686567164178 0.07490645511936117\nEpoch: 163 Training Accuracy: 0.5966 Training Loss: 1.4877 Validation Accuracy: 0.0917 Validation Loss: 9.0439 \n0.08983357024401799 0.06716417910447761 0.059523112727678136\nEpoch: 164 Training Accuracy: 0.5607 Training Loss: 1.4396 Validation Accuracy: 0.0764 Validation Loss: 7.9964 \n0.03321864702461717 0.05223880597014925 0.037882483009787665\nEpoch: 165 Training Accuracy: 0.5199 Training Loss: 1.5079 Validation Accuracy: 0.0764 Validation Loss: 6.8010 \n0.021515797635200617 0.03731343283582089 0.027052238805970148\nEpoch: 166 Training Accuracy: 0.5527 Training Loss: 1.4955 Validation Accuracy: 0.0229 Validation Loss: 7.9313 \n0.03732124606171181 0.03731343283582089 0.033211484575354874\nEpoch: 167 Training Accuracy: 0.5713 Training Loss: 1.5292 Validation Accuracy: 0.0382 Validation Loss: 8.0786 \n0.03227977758267486 0.05223880597014925 0.03532164353059875\nEpoch: 168 Training Accuracy: 0.5561 Training Loss: 1.5117 Validation Accuracy: 0.0458 Validation Loss: 7.8358 \n0.09515549343583088 0.06716417910447761 0.06545990286661929\nEpoch: 169 Training Accuracy: 0.5681 Training Loss: 1.5615 Validation Accuracy: 0.0382 Validation Loss: 7.8668 \n0.047609801670381126 0.04477611940298507 0.0427672244836424\nEpoch: 170 Training Accuracy: 0.5581 Training Loss: 1.5701 Validation Accuracy: 0.0535 Validation Loss: 8.0389 \n0.08066684123243431 0.08208955223880597 0.07232908097401419\nEpoch: 171 Training Accuracy: 0.5338 Training Loss: 1.6004 Validation Accuracy: 0.0382 Validation Loss: 6.9730 \n0.03828932261768083 0.05223880597014925 0.04295486851457001\nEpoch: 172 Training Accuracy: 0.5852 Training Loss: 1.4498 Validation Accuracy: 0.0458 Validation Loss: 7.1432 \n0.04563317864459216 0.06716417910447761 0.05097634357854994\nEpoch: 173 Training Accuracy: 0.5051 Training Loss: 1.7436 Validation Accuracy: 0.0382 Validation Loss: 6.3903 \n0.05477078891257996 0.04477611940298507 0.045124517468679015\nEpoch: 174 Training Accuracy: 0.4886 Training Loss: 1.6523 Validation Accuracy: 0.0458 Validation Loss: 6.6665 \n0.09158040442946558 0.06716417910447761 0.06896888108477223\nEpoch: 175 Training Accuracy: 0.4624 Training Loss: 1.7125 Validation Accuracy: 0.0535 Validation Loss: 6.7928 \n0.06463946501259935 0.05970149253731343 0.05607618530138284\nEpoch: 176 Training Accuracy: 0.5354 Training Loss: 1.5040 Validation Accuracy: 0.0764 Validation Loss: 7.0530 \n0.11094053541814736 0.09701492537313433 0.09514811351949193\nEpoch: 177 Training Accuracy: 0.5006 Training Loss: 1.5316 Validation Accuracy: 0.0611 Validation Loss: 7.7391 \n0.09917673537076523 0.06716417910447761 0.07197131226981973\nEpoch: 178 Training Accuracy: 0.5491 Training Loss: 1.5721 Validation Accuracy: 0.0535 Validation Loss: 7.6627 \n0.07815319009348862 0.07462686567164178 0.06699186093391538\nEpoch: 179 Training Accuracy: 0.5156 Training Loss: 1.6006 Validation Accuracy: 0.0306 Validation Loss: 6.2352 \n0.03174141255445262 0.04477611940298507 0.0295726612671301\nEpoch: 180 Training Accuracy: 0.5219 Training Loss: 1.5570 Validation Accuracy: 0.0611 Validation Loss: 5.6258 \n0.02703298981284056 0.05223880597014925 0.03359495178819609\nEpoch: 181 Training Accuracy: 0.4260 Training Loss: 1.9781 Validation Accuracy: 0.0229 Validation Loss: 5.9981 \n0.02106755185113394 0.029850746268656716 0.02438936539024335\nEpoch: 182 Training Accuracy: 0.5075 Training Loss: 1.8978 Validation Accuracy: 0.0535 Validation Loss: 5.9672 \n0.07778154681139755 0.05970149253731343 0.05945319191587848\nEpoch: 183 Training Accuracy: 0.4863 Training Loss: 1.8403 Validation Accuracy: 0.0382 Validation Loss: 5.8664 \n0.050728500355366024 0.03731343283582089 0.03410886742756804\nEpoch: 184 Training Accuracy: 0.3906 Training Loss: 2.1762 Validation Accuracy: 0.0535 Validation Loss: 5.6262 \n0.06247927031509121 0.05223880597014925 0.054411764705882354\nEpoch: 185 Training Accuracy: 0.3877 Training Loss: 2.1258 Validation Accuracy: 0.0764 Validation Loss: 6.0433 \n0.07893864013266998 0.06716417910447761 0.06478322672352523\nEpoch: 186 Training Accuracy: 0.4123 Training Loss: 1.8962 Validation Accuracy: 0.0687 Validation Loss: 6.4191 \n0.093945544505246 0.05970149253731343 0.05267457815456984\nEpoch: 187 Training Accuracy: 0.3612 Training Loss: 2.3942 Validation Accuracy: 0.0382 Validation Loss: 5.8123 \n0.04650179836747001 0.04477611940298507 0.03359640054455452\nEpoch: 188 Training Accuracy: 0.3467 Training Loss: 2.3367 Validation Accuracy: 0.0382 Validation Loss: 5.9764 \n0.07647157982978878 0.06716417910447761 0.05395643919386431\nEpoch: 189 Training Accuracy: 0.3564 Training Loss: 2.3606 Validation Accuracy: 0.0687 Validation Loss: 5.8060 \n0.0840529070301262 0.05970149253731343 0.052518157873732936\nEpoch: 190 Training Accuracy: 0.3072 Training Loss: 2.3906 Validation Accuracy: 0.0382 Validation Loss: 4.3311 \n0.04341926729986432 0.022388059701492536 0.027860696517412936\nEpoch: 191 Training Accuracy: 0.2659 Training Loss: 2.5612 Validation Accuracy: 0.0306 Validation Loss: 4.2684 \n0.006143065161994904 0.014925373134328358 0.007414227563481295\nEpoch: 192 Training Accuracy: 0.2625 Training Loss: 2.5979 Validation Accuracy: 0.0611 Validation Loss: 3.8278 \n0.07290193768002096 0.05970149253731343 0.04827944023080871\nEpoch: 193 Training Accuracy: 0.2138 Training Loss: 2.8548 Validation Accuracy: 0.0611 Validation Loss: 3.6439 \n0.04067660104580765 0.05223880597014925 0.0334186413599249\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[72], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m      6\u001b[0m     model2\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m     train_loss_epoch, train_accuracy_epoch, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_train2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     train_accuracy_epoch \u001b[38;5;241m=\u001b[39m train_accuracy_epoch \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader2)\n\u001b[1;32m      9\u001b[0m     train_loss_epoch \u001b[38;5;241m=\u001b[39m train_loss_epoch \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader2)\n","Cell \u001b[0;32mIn[62], line 12\u001b[0m, in \u001b[0;36mrun_train2\u001b[0;34m(model2, optimizer2, loader2, criterion2, flag2)\u001b[0m\n\u001b[1;32m     10\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m criterion2(predicted_y, ground_truth)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag2:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtraining_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     optimizer2\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m train_loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m training_loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"minimum_training_loss = np.inf\ntrain_loss, validation_loss = list(), list()\nprev_f1 = 0\n\nfor epoch in range(100):\n    model2.train()\n    train_loss_epoch, train_accuracy_epoch, train_loss = run_train2(model2, optimizer2, train_loader21, criterion2, True)\n    train_accuracy_epoch = train_accuracy_epoch / len(train_loader21)\n    train_loss_epoch = train_loss_epoch / len(train_loader21)\n    model2.eval()\n    validation_loss_epoch, validation_accuracy_epoch, validation_loss = run_train2(model2, optimizer2, validation_loader21, criterion2, False)\n    validation_accuracy_epoch = validation_accuracy_epoch / len(validation_loader21)\n    validation_loss_epoch = validation_loss_epoch / len(validation_loader21)\n    print(\"Epoch: {:.0f} Training Accuracy: {:.4f} Training Loss: {:.4f} Validation Accuracy: {:.4f} Validation Loss: {:.4f} \".format(epoch,train_accuracy_epoch,train_loss_epoch,validation_accuracy_epoch,validation_loss_epoch))\n\n    prediction_list, ground_truth_list = get_prediction_list2(model2, validation_loader21_,X_val,idx_to_cluster)\n#     print(prediction_list, ground_truth_list)\n    precision, recall, f1,_ = precision_recall_fscore_support (ground_truth_list, prediction_list, average='weighted',zero_division=0)\n    print(precision, recall,  f1)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:02:33.037189Z","iopub.execute_input":"2023-11-29T07:02:33.037929Z","iopub.status.idle":"2023-11-29T07:14:05.309357Z","shell.execute_reply.started":"2023-11-29T07:02:33.037896Z","shell.execute_reply":"2023-11-29T07:14:05.308346Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Epoch: 0 Training Accuracy: 0.2054 Training Loss: 2.5379 Validation Accuracy: 0.0306 Validation Loss: 4.2033 \n0.28955223880597014 0.014925373134328358 0.02632669983416252\nEpoch: 1 Training Accuracy: 0.2180 Training Loss: 2.5932 Validation Accuracy: 0.0572 Validation Loss: 4.1359 \n0.125 0.014925373134328358 0.026652452025586356\nEpoch: 2 Training Accuracy: 0.2153 Training Loss: 2.5317 Validation Accuracy: 0.0687 Validation Loss: 3.6330 \n0.30149253731343284 0.04477611940298507 0.07794361525704809\nEpoch: 3 Training Accuracy: 0.2306 Training Loss: 2.6225 Validation Accuracy: 0.0535 Validation Loss: 3.6382 \n0.181592039800995 0.014925373134328358 0.027580326863755673\nEpoch: 4 Training Accuracy: 0.1767 Training Loss: 2.6044 Validation Accuracy: 0.0535 Validation Loss: 3.7072 \n0.0773405698778833 0.022388059701492536 0.0347243374961925\nEpoch: 5 Training Accuracy: 0.1793 Training Loss: 2.7403 Validation Accuracy: 0.0535 Validation Loss: 3.5450 \n0.17537313432835822 0.029850746268656716 0.05077871512005191\nEpoch: 6 Training Accuracy: 0.1793 Training Loss: 2.6597 Validation Accuracy: 0.0687 Validation Loss: 3.7235 \n0.14298507462686566 0.06716417910447761 0.08508054963278844\nEpoch: 7 Training Accuracy: 0.1960 Training Loss: 2.6714 Validation Accuracy: 0.0229 Validation Loss: 3.6663 \n0.3117106773823192 0.05223880597014925 0.08435762364647352\nEpoch: 8 Training Accuracy: 0.1637 Training Loss: 2.7666 Validation Accuracy: 0.0382 Validation Loss: 3.4202 \n0.0914179104477612 0.05223880597014925 0.06648575305291722\nEpoch: 9 Training Accuracy: 0.1525 Training Loss: 2.7131 Validation Accuracy: 0.0382 Validation Loss: 3.3078 \n0.1424129353233831 0.03731343283582089 0.05458004097161252\nEpoch: 10 Training Accuracy: 0.1518 Training Loss: 2.6874 Validation Accuracy: 0.0382 Validation Loss: 3.2838 \n0.11567164179104478 0.007462686567164179 0.014020805065581185\nEpoch: 11 Training Accuracy: 0.1566 Training Loss: 2.5776 Validation Accuracy: 0.0535 Validation Loss: 3.4264 \n0.1579601990049751 0.029850746268656716 0.05014165651065985\nEpoch: 12 Training Accuracy: 0.1701 Training Loss: 2.6680 Validation Accuracy: 0.0382 Validation Loss: 3.3841 \n0.0 0.0 0.0\nEpoch: 13 Training Accuracy: 0.2059 Training Loss: 2.4200 Validation Accuracy: 0.0458 Validation Loss: 3.4054 \n0.19179104477611938 0.029850746268656716 0.051283759772565746\nEpoch: 14 Training Accuracy: 0.1644 Training Loss: 2.6009 Validation Accuracy: 0.0419 Validation Loss: 3.3228 \n0.373134328358209 0.05223880597014925 0.0894238879313506\nEpoch: 15 Training Accuracy: 0.1947 Training Loss: 2.5137 Validation Accuracy: 0.0306 Validation Loss: 3.3237 \n0.4962686567164179 0.05223880597014925 0.09358145465511859\nEpoch: 16 Training Accuracy: 0.2002 Training Loss: 2.4418 Validation Accuracy: 0.0229 Validation Loss: 3.4129 \n0.1828358208955224 0.014925373134328358 0.027453640886476706\nEpoch: 17 Training Accuracy: 0.2215 Training Loss: 2.3293 Validation Accuracy: 0.0306 Validation Loss: 3.5203 \n0.29850746268656714 0.029850746268656716 0.05425812115891133\nEpoch: 18 Training Accuracy: 0.2138 Training Loss: 2.2922 Validation Accuracy: 0.0306 Validation Loss: 3.5224 \n0.23134328358208955 0.03731343283582089 0.06426202321724708\nEpoch: 19 Training Accuracy: 0.2439 Training Loss: 2.2632 Validation Accuracy: 0.0306 Validation Loss: 3.5186 \n0.17350746268656717 0.022388059701492536 0.039658848614072484\nEpoch: 20 Training Accuracy: 0.2572 Training Loss: 2.2347 Validation Accuracy: 0.0382 Validation Loss: 3.6655 \n0.15597014925373134 0.014925373134328358 0.026614088647670738\nEpoch: 21 Training Accuracy: 0.2798 Training Loss: 2.1383 Validation Accuracy: 0.0535 Validation Loss: 3.6858 \n0.39552238805970147 0.029850746268656716 0.05345975541871064\nEpoch: 22 Training Accuracy: 0.2717 Training Loss: 2.1946 Validation Accuracy: 0.0343 Validation Loss: 3.6172 \n0.15597014925373134 0.014925373134328358 0.026614088647670738\nEpoch: 23 Training Accuracy: 0.2714 Training Loss: 2.1574 Validation Accuracy: 0.0382 Validation Loss: 3.6936 \n0.29850746268656714 0.022388059701492536 0.03888229308005428\nEpoch: 24 Training Accuracy: 0.3136 Training Loss: 2.0388 Validation Accuracy: 0.0382 Validation Loss: 3.9960 \n0.05037313432835821 0.007462686567164179 0.012999518536350506\nEpoch: 25 Training Accuracy: 0.2949 Training Loss: 2.0999 Validation Accuracy: 0.0535 Validation Loss: 3.9290 \n0.13059701492537312 0.007462686567164179 0.014118596208148445\nEpoch: 26 Training Accuracy: 0.3216 Training Loss: 2.0131 Validation Accuracy: 0.0382 Validation Loss: 3.8959 \n0.13059701492537312 0.022388059701492536 0.038223516563523845\nEpoch: 27 Training Accuracy: 0.2942 Training Loss: 2.0736 Validation Accuracy: 0.0382 Validation Loss: 3.9631 \n0.30149253731343284 0.014925373134328358 0.027104063018242124\nEpoch: 28 Training Accuracy: 0.2620 Training Loss: 2.1043 Validation Accuracy: 0.0306 Validation Loss: 3.9786 \n0.1708955223880597 0.014925373134328358 0.026711879790237997\nEpoch: 29 Training Accuracy: 0.3239 Training Loss: 2.0823 Validation Accuracy: 0.0382 Validation Loss: 4.0379 \n0.12736318407960198 0.014925373134328358 0.026340337784760408\nEpoch: 30 Training Accuracy: 0.3285 Training Loss: 2.0328 Validation Accuracy: 0.0458 Validation Loss: 3.9496 \n0.0 0.0 0.0\nEpoch: 31 Training Accuracy: 0.2746 Training Loss: 2.0213 Validation Accuracy: 0.0458 Validation Loss: 3.8455 \n0.20771144278606965 0.014925373134328358 0.02772702465415371\nEpoch: 32 Training Accuracy: 0.3055 Training Loss: 2.0102 Validation Accuracy: 0.0535 Validation Loss: 3.5830 \n0.11567164179104478 0.007462686567164179 0.014020805065581185\nEpoch: 33 Training Accuracy: 0.3392 Training Loss: 1.9609 Validation Accuracy: 0.0382 Validation Loss: 3.6822 \n0.2027363184079602 0.014925373134328358 0.02776785926825204\nEpoch: 34 Training Accuracy: 0.3195 Training Loss: 1.9821 Validation Accuracy: 0.0229 Validation Loss: 3.9114 \n0.166044776119403 0.014925373134328358 0.027020323601931694\nEpoch: 35 Training Accuracy: 0.3375 Training Loss: 1.9179 Validation Accuracy: 0.0382 Validation Loss: 3.9931 \n0.33880597014925373 0.03731343283582089 0.06595149253731343\nEpoch: 36 Training Accuracy: 0.3377 Training Loss: 1.9514 Validation Accuracy: 0.0458 Validation Loss: 3.8479 \n0.2251243781094527 0.029850746268656716 0.052056789224608664\nEpoch: 37 Training Accuracy: 0.3063 Training Loss: 1.8985 Validation Accuracy: 0.0458 Validation Loss: 4.1611 \n0.3414179104477612 0.014925373134328358 0.02776228746378\nEpoch: 38 Training Accuracy: 0.3542 Training Loss: 1.8736 Validation Accuracy: 0.0458 Validation Loss: 4.2398 \n0.40298507462686567 0.014925373134328358 0.028689883913764515\nEpoch: 39 Training Accuracy: 0.3974 Training Loss: 1.7766 Validation Accuracy: 0.0535 Validation Loss: 4.4232 \n0.2789179104477612 0.04477611940298507 0.07476360270528962\nEpoch: 40 Training Accuracy: 0.3896 Training Loss: 1.8132 Validation Accuracy: 0.0535 Validation Loss: 4.5059 \n0.33830845771144274 0.029850746268656716 0.05484959105401784\nEpoch: 41 Training Accuracy: 0.3480 Training Loss: 1.8458 Validation Accuracy: 0.0535 Validation Loss: 4.4367 \n0.376865671641791 0.029850746268656716 0.05467642482567856\nEpoch: 42 Training Accuracy: 0.3871 Training Loss: 1.7514 Validation Accuracy: 0.0611 Validation Loss: 4.2352 \n0.39104477611940297 0.05223880597014925 0.09189173181710496\nEpoch: 43 Training Accuracy: 0.4068 Training Loss: 1.8126 Validation Accuracy: 0.0535 Validation Loss: 4.1995 \n0.2730099502487562 0.029850746268656716 0.05379212534612007\nEpoch: 44 Training Accuracy: 0.4206 Training Loss: 1.7247 Validation Accuracy: 0.0535 Validation Loss: 3.9993 \n0.3880597014925373 0.029850746268656716 0.05363805970149254\nEpoch: 45 Training Accuracy: 0.3679 Training Loss: 1.8117 Validation Accuracy: 0.0458 Validation Loss: 4.1513 \n0.08706467661691542 0.014925373134328358 0.02548234437568256\nEpoch: 46 Training Accuracy: 0.3966 Training Loss: 1.7380 Validation Accuracy: 0.0535 Validation Loss: 4.2589 \n0.13059701492537312 0.007462686567164179 0.014118596208148445\nEpoch: 47 Training Accuracy: 0.3924 Training Loss: 1.7469 Validation Accuracy: 0.0611 Validation Loss: 4.1357 \n0.3843283582089552 0.029850746268656716 0.05534818613200772\nEpoch: 48 Training Accuracy: 0.4432 Training Loss: 1.6854 Validation Accuracy: 0.0535 Validation Loss: 4.2644 \n0.1884328358208955 0.014925373134328358 0.02733821241283928\nEpoch: 49 Training Accuracy: 0.3985 Training Loss: 1.7480 Validation Accuracy: 0.0611 Validation Loss: 4.3716 \n0.2462686567164179 0.022388059701492536 0.04080993633232439\nEpoch: 50 Training Accuracy: 0.3912 Training Loss: 1.7498 Validation Accuracy: 0.0611 Validation Loss: 4.4897 \n0.27238805970149255 0.029850746268656716 0.053199909543193126\nEpoch: 51 Training Accuracy: 0.4363 Training Loss: 1.7158 Validation Accuracy: 0.0535 Validation Loss: 4.5533 \n0.4925373134328358 0.014925373134328358 0.028969734660033167\nEpoch: 52 Training Accuracy: 0.4012 Training Loss: 1.6904 Validation Accuracy: 0.0611 Validation Loss: 4.7224 \n0.47139303482587064 0.022388059701492536 0.04236485716281858\nEpoch: 53 Training Accuracy: 0.4436 Training Loss: 1.5810 Validation Accuracy: 0.0458 Validation Loss: 4.9779 \n0.40298507462686567 0.029850746268656716 0.05524501046889107\nEpoch: 54 Training Accuracy: 0.4279 Training Loss: 1.7058 Validation Accuracy: 0.0382 Validation Loss: 4.9655 \n0.4274875621890547 0.06716417910447761 0.1151767309693973\nEpoch: 55 Training Accuracy: 0.4355 Training Loss: 1.5921 Validation Accuracy: 0.0306 Validation Loss: 4.7426 \n0.47636815920398007 0.05223880597014925 0.09318799216678243\nEpoch: 56 Training Accuracy: 0.4515 Training Loss: 1.5896 Validation Accuracy: 0.0382 Validation Loss: 4.6513 \n0.4208955223880597 0.04477611940298507 0.08037880771580692\nEpoch: 57 Training Accuracy: 0.4720 Training Loss: 1.5414 Validation Accuracy: 0.0458 Validation Loss: 4.5273 \n0.11567164179104478 0.022388059701492536 0.037515127067365865\nEpoch: 58 Training Accuracy: 0.4435 Training Loss: 1.7705 Validation Accuracy: 0.0458 Validation Loss: 4.4472 \n0.376865671641791 0.022388059701492536 0.040950011845534234\nEpoch: 59 Training Accuracy: 0.4514 Training Loss: 1.6481 Validation Accuracy: 0.0458 Validation Loss: 4.7011 \n0.3537313432835821 0.029850746268656716 0.05394200170319573\nEpoch: 60 Training Accuracy: 0.4756 Training Loss: 1.5793 Validation Accuracy: 0.0535 Validation Loss: 4.8025 \n0.44029850746268656 0.03731343283582089 0.06669776119402986\nEpoch: 61 Training Accuracy: 0.4476 Training Loss: 1.6773 Validation Accuracy: 0.0382 Validation Loss: 4.5501 \n0.3757995735607676 0.05970149253731343 0.10252158623429955\nEpoch: 62 Training Accuracy: 0.4388 Training Loss: 1.5447 Validation Accuracy: 0.0382 Validation Loss: 4.3027 \n0.16324626865671643 0.03731343283582089 0.06074279763970843\nEpoch: 63 Training Accuracy: 0.4286 Training Loss: 1.6186 Validation Accuracy: 0.0458 Validation Loss: 4.5155 \n0.35572139303482586 0.014925373134328358 0.02834405209723739\nEpoch: 64 Training Accuracy: 0.4723 Training Loss: 1.6683 Validation Accuracy: 0.0535 Validation Loss: 4.4712 \n0.32611940298507464 0.04477611940298507 0.07859511963989574\nEpoch: 65 Training Accuracy: 0.4612 Training Loss: 1.6041 Validation Accuracy: 0.0611 Validation Loss: 4.8522 \n0.24440298507462688 0.04477611940298507 0.07540867093105899\nEpoch: 66 Training Accuracy: 0.4492 Training Loss: 1.6208 Validation Accuracy: 0.0535 Validation Loss: 4.8963 \n0.2748756218905472 0.04477611940298507 0.07402080506558117\nEpoch: 67 Training Accuracy: 0.4708 Training Loss: 1.5555 Validation Accuracy: 0.0535 Validation Loss: 4.8735 \n0.15111940298507462 0.014925373134328358 0.02635046113306983\nEpoch: 68 Training Accuracy: 0.4563 Training Loss: 1.6023 Validation Accuracy: 0.0535 Validation Loss: 5.0797 \n0.22761194029850745 0.029850746268656716 0.05133423790140208\nEpoch: 69 Training Accuracy: 0.4614 Training Loss: 1.5197 Validation Accuracy: 0.0535 Validation Loss: 5.3441 \n0.181592039800995 0.022388059701492536 0.039727831431079885\nEpoch: 70 Training Accuracy: 0.4686 Training Loss: 1.6439 Validation Accuracy: 0.0687 Validation Loss: 5.2235 \n0.23175787728026534 0.03731343283582089 0.06151062867480778\nEpoch: 71 Training Accuracy: 0.4744 Training Loss: 1.4694 Validation Accuracy: 0.0687 Validation Loss: 6.1940 \n0.13805970149253732 0.014925373134328358 0.026936744847192613\nEpoch: 72 Training Accuracy: 0.4628 Training Loss: 1.5347 Validation Accuracy: 0.0687 Validation Loss: 5.8472 \n0.3532338308457711 0.03731343283582089 0.06712777751347579\nEpoch: 73 Training Accuracy: 0.4760 Training Loss: 1.5467 Validation Accuracy: 0.0687 Validation Loss: 5.5756 \n0.13805970149253732 0.014925373134328358 0.026936744847192613\nEpoch: 74 Training Accuracy: 0.5100 Training Loss: 1.4249 Validation Accuracy: 0.0535 Validation Loss: 5.3628 \n0.31156716417910446 0.014925373134328358 0.027510297972503078\nEpoch: 75 Training Accuracy: 0.5133 Training Loss: 1.4234 Validation Accuracy: 0.0382 Validation Loss: 5.4535 \n0.13059701492537312 0.014925373134328358 0.026789131266743205\nEpoch: 76 Training Accuracy: 0.5062 Training Loss: 1.3679 Validation Accuracy: 0.0382 Validation Loss: 5.6685 \n0.17412935323383083 0.014925373134328358 0.02749410840534171\nEpoch: 77 Training Accuracy: 0.4768 Training Loss: 1.3998 Validation Accuracy: 0.0458 Validation Loss: 5.5998 \n0.13059701492537312 0.007462686567164179 0.014118596208148445\nEpoch: 78 Training Accuracy: 0.5282 Training Loss: 1.3634 Validation Accuracy: 0.0458 Validation Loss: 5.6580 \n0.13059701492537312 0.014925373134328358 0.026789131266743205\nEpoch: 79 Training Accuracy: 0.5265 Training Loss: 1.3461 Validation Accuracy: 0.0458 Validation Loss: 5.8567 \n0.18731343283582091 0.022388059701492536 0.03997899589707989\nEpoch: 80 Training Accuracy: 0.5479 Training Loss: 1.3202 Validation Accuracy: 0.0535 Validation Loss: 5.9909 \n0.42661691542288555 0.04477611940298507 0.07839603061663492\nEpoch: 81 Training Accuracy: 0.5214 Training Loss: 1.3924 Validation Accuracy: 0.0535 Validation Loss: 5.8798 \n0.27238805970149255 0.03731343283582089 0.06277334258937868\nEpoch: 82 Training Accuracy: 0.5310 Training Loss: 1.3356 Validation Accuracy: 0.0611 Validation Loss: 5.6020 \n0.08706467661691542 0.014925373134328358 0.02548234437568256\nEpoch: 83 Training Accuracy: 0.5434 Training Loss: 1.3221 Validation Accuracy: 0.0458 Validation Loss: 5.9891 \n0.11194029850746269 0.022388059701492536 0.03731343283582089\nEpoch: 84 Training Accuracy: 0.5459 Training Loss: 1.3199 Validation Accuracy: 0.0535 Validation Loss: 6.3024 \n0.13059701492537312 0.029850746268656716 0.04859423811176674\nEpoch: 85 Training Accuracy: 0.5576 Training Loss: 1.3198 Validation Accuracy: 0.0382 Validation Loss: 6.4467 \n0.13059701492537312 0.022388059701492536 0.038223516563523845\nEpoch: 86 Training Accuracy: 0.5778 Training Loss: 1.2265 Validation Accuracy: 0.0306 Validation Loss: 6.3575 \n0.26119402985074625 0.007462686567164179 0.014510779436152572\nEpoch: 87 Training Accuracy: 0.5119 Training Loss: 1.2993 Validation Accuracy: 0.0917 Validation Loss: 6.4009 \n0.22761194029850745 0.029850746268656716 0.05133423790140208\nEpoch: 88 Training Accuracy: 0.5656 Training Loss: 1.3447 Validation Accuracy: 0.0764 Validation Loss: 6.5251 \n0.26119402985074625 0.007462686567164179 0.014510779436152572\nEpoch: 89 Training Accuracy: 0.5794 Training Loss: 1.2680 Validation Accuracy: 0.0840 Validation Loss: 6.6101 \n0.4324626865671642 0.029850746268656716 0.05411012490151678\nEpoch: 90 Training Accuracy: 0.5537 Training Loss: 1.2719 Validation Accuracy: 0.0764 Validation Loss: 6.3794 \n0.4324626865671642 0.03731343283582089 0.06783653788166111\nEpoch: 91 Training Accuracy: 0.5324 Training Loss: 1.3402 Validation Accuracy: 0.0840 Validation Loss: 6.4217 \n0.20460199004975121 0.029850746268656716 0.05205950765652258\nEpoch: 92 Training Accuracy: 0.5686 Training Loss: 1.2503 Validation Accuracy: 0.0917 Validation Loss: 6.0819 \n0.17288557213930347 0.029850746268656716 0.0508774817111184\nEpoch: 93 Training Accuracy: 0.5709 Training Loss: 1.2352 Validation Accuracy: 0.0840 Validation Loss: 6.1604 \n0.17686567164179104 0.029850746268656716 0.05107592120697327\nEpoch: 94 Training Accuracy: 0.5657 Training Loss: 1.3171 Validation Accuracy: 0.0764 Validation Loss: 6.6434 \n0.14378109452736318 0.014925373134328358 0.026936918833007546\nEpoch: 95 Training Accuracy: 0.5657 Training Loss: 1.2804 Validation Accuracy: 0.1069 Validation Loss: 6.5991 \n0.16119402985074627 0.022388059701492536 0.039309267615411314\nEpoch: 96 Training Accuracy: 0.5901 Training Loss: 1.1901 Validation Accuracy: 0.0535 Validation Loss: 6.6939 \n0.08706467661691542 0.014925373134328358 0.02548234437568256\nEpoch: 97 Training Accuracy: 0.5944 Training Loss: 1.2822 Validation Accuracy: 0.0917 Validation Loss: 6.7734 \n0.08706467661691542 0.014925373134328358 0.02548234437568256\nEpoch: 98 Training Accuracy: 0.5649 Training Loss: 1.2401 Validation Accuracy: 0.0840 Validation Loss: 6.6848 \n0.08706467661691542 0.014925373134328358 0.02548234437568256\nEpoch: 99 Training Accuracy: 0.5575 Training Loss: 1.2512 Validation Accuracy: 0.0840 Validation Loss: 6.4569 \n0.13059701492537312 0.014925373134328358 0.026789131266743205\n","output_type":"stream"}]}]}